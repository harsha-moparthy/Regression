{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":72489,"databundleVersionId":8096274,"sourceType":"competition"},{"sourceId":57419,"sourceType":"datasetVersion","datasetId":37691}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install autogluon.tabular\n!pip install ucimlrepo\n!pip install openfe","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T09:50:06.563432Z","iopub.execute_input":"2024-05-18T09:50:06.564089Z","iopub.status.idle":"2024-05-18T09:50:56.622870Z","shell.execute_reply.started":"2024-05-18T09:50:06.564056Z","shell.execute_reply":"2024-05-18T09:50:56.621693Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting autogluon.tabular\n  Downloading autogluon.tabular-1.1.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<1.29,>=1.21 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (1.26.4)\nRequirement already satisfied: scipy<1.13,>=1.5.4 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (1.11.4)\nRequirement already satisfied: pandas<2.3.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (2.1.4)\nCollecting scikit-learn<1.4.1,>=1.3.0 (from autogluon.tabular)\n  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (3.2.1)\nCollecting autogluon.core==1.1.0 (from autogluon.tabular)\n  Downloading autogluon.core-1.1.0-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.features==1.1.0 (from autogluon.tabular)\n  Downloading autogluon.features-1.1.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.0->autogluon.tabular) (4.66.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.0->autogluon.tabular) (2.31.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.0->autogluon.tabular) (3.7.5)\nRequirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.0->autogluon.tabular) (1.26.100)\nCollecting autogluon.common==1.1.0 (from autogluon.core==1.1.0->autogluon.tabular)\n  Downloading autogluon.common-1.1.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: psutil<6,>=5.7.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.0->autogluon.core==1.1.0->autogluon.tabular) (5.9.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.0->autogluon.core==1.1.0->autogluon.tabular) (69.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2023.4)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (3.2.0)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3<2,>=1.10->autogluon.core==1.1.0->autogluon.tabular)\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.0->autogluon.tabular) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.0->autogluon.tabular) (0.6.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular) (1.16.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.0->autogluon.tabular) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.0->autogluon.tabular) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.0->autogluon.tabular) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.0->autogluon.tabular) (2024.2.2)\nDownloading autogluon.tabular-1.1.0-py3-none-any.whl (308 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.core-1.1.0-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.features-1.1.0-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.common-1.1.0-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn, botocore, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.34.69\n    Uninstalling botocore-1.34.69:\n      Successfully uninstalled botocore-1.34.69\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.29.165 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed autogluon.common-1.1.0 autogluon.core-1.1.0 autogluon.features-1.1.0 autogluon.tabular-1.1.0 botocore-1.29.165 scikit-learn-1.4.0\nCollecting ucimlrepo\n  Downloading ucimlrepo-0.0.6-py3-none-any.whl.metadata (5.3 kB)\nDownloading ucimlrepo-0.0.6-py3-none-any.whl (8.0 kB)\nInstalling collected packages: ucimlrepo\nSuccessfully installed ucimlrepo-0.0.6\nCollecting openfe\n  Downloading openfe-0.0.12-py3-none-any.whl.metadata (667 bytes)\nRequirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.10/site-packages (from openfe) (1.26.4)\nRequirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.10/site-packages (from openfe) (2.1.4)\nRequirement already satisfied: scikit-learn>=0.24.2 in /opt/conda/lib/python3.10/site-packages (from openfe) (1.4.0)\nRequirement already satisfied: lightgbm>=3.3.2 in /opt/conda/lib/python3.10/site-packages (from openfe) (4.2.0)\nRequirement already satisfied: scipy>=1.5.4 in /opt/conda/lib/python3.10/site-packages (from openfe) (1.11.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openfe) (4.66.1)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from openfe) (15.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->openfe) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->openfe) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->openfe) (2023.4)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->openfe) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->openfe) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->openfe) (1.16.0)\nDownloading openfe-0.0.12-py3-none-any.whl (21 kB)\nInstalling collected packages: openfe\nSuccessfully installed openfe-0.0.12\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns',None)\npd.set_option('display.width',100)\npd.set_option('display.max_rows',200)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:53:29.861655Z","iopub.execute_input":"2024-05-18T09:53:29.862559Z","iopub.status.idle":"2024-05-18T09:53:31.770528Z","shell.execute_reply.started":"2024-05-18T09:53:29.862523Z","shell.execute_reply":"2024-05-18T09:53:31.769588Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from ucimlrepo import fetch_ucirepo \n  \n# fetch dataset \nabalone = fetch_ucirepo(id=1) \n  \n# data (as pandas dataframes)\norig_train = pd.concat([abalone.data.features, abalone.data.targets], axis = 1).rename({\n    'Whole_weight' : 'Whole weight',\n    'Shucked_weight' : 'Whole weight.1',\n    'Viscera_weight' : 'Whole weight.2',\n    'Shell_weight' : 'Shell weight'\n}, axis = 1)\norig_train.to_csv('original_train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:53:36.162265Z","iopub.execute_input":"2024-05-18T09:53:36.163497Z","iopub.status.idle":"2024-05-18T09:53:37.984829Z","shell.execute_reply.started":"2024-05-18T09:53:36.163464Z","shell.execute_reply":"2024-05-18T09:53:37.984107Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s4e4/train.csv\",index_col='id')\ntest = pd.read_csv(\"/kaggle/input/playground-series-s4e4/test.csv\",index_col='id')\nsubmission = pd.read_csv(\"/kaggle/input/playground-series-s4e4/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:53:45.250420Z","iopub.execute_input":"2024-05-18T09:53:45.250773Z","iopub.status.idle":"2024-05-18T09:53:45.542365Z","shell.execute_reply.started":"2024-05-18T09:53:45.250743Z","shell.execute_reply":"2024-05-18T09:53:45.541347Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train,orig_train],ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:56:23.132490Z","iopub.execute_input":"2024-05-18T09:56:23.133202Z","iopub.status.idle":"2024-05-18T09:56:23.143610Z","shell.execute_reply.started":"2024-05-18T09:56:23.133173Z","shell.execute_reply":"2024-05-18T09:56:23.142748Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def feats(df):\n    df['Height'] = np.where(df['Height']==0,0.005,df['Height'])\n    return df\ntrain = feats(train)\ntest = feats(test)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:59:17.592614Z","iopub.execute_input":"2024-05-18T09:59:17.593484Z","iopub.status.idle":"2024-05-18T09:59:17.600945Z","shell.execute_reply.started":"2024-05-18T09:59:17.593451Z","shell.execute_reply":"2024-05-18T09:59:17.599995Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from openfe import OpenFE, transform\n\nofe = OpenFE()\n\nfeatures = ofe.fit(data=train.drop('Rings',axis=1),label=train['Rings'],feature_boosting=True,n_jobs=3)\ntrain_x, test_x = transform(train.drop('Rings',axis=1),test,features,n_jobs=4)\ntrain_x.reset_index(drop=True, inplace=True)\ntrain = pd.concat([train_x,train['Rings']],axis=1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:59:22.863979Z","iopub.execute_input":"2024-05-18T09:59:22.864333Z","iopub.status.idle":"2024-05-18T10:00:36.042203Z","shell.execute_reply.started":"2024-05-18T09:59:22.864304Z","shell.execute_reply":"2024-05-18T10:00:36.041113Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007916 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1331\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 9.718698\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[390]\tvalid_0's rmse: 1.83664\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005942 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1331\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 9.698588\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[286]\tvalid_0's rmse: 1.87268\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011114 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1331\n[LightGBM] [Info] Number of data points in the train set: 75834, number of used features: 8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 9.711831\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[357]\tvalid_0's rmse: 1.82877\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005873 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1333\n[LightGBM] [Info] Number of data points in the train set: 75834, number of used features: 8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 9.718833\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[389]\tvalid_0's rmse: 1.7752\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005664 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1330\n[LightGBM] [Info] Number of data points in the train set: 75834, number of used features: 8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 9.688214\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[376]\tvalid_0's rmse: 1.879\nThe number of candidate features is 266\nStart stage I selection.\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/12 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000335 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 253\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 104\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000337 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000313 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000189 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 46\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000262 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Total Bins 104[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 238\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000219 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 47\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 97\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000185 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000324 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255[LightGBM] [Info] Total Bins 4\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000277 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 74\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000181 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 35\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines[LightGBM] [Info] Total Bins 47\n\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 93[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000185 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 4\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n\n[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Total Bins 0[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 104\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000188 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 4\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000066 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000275 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000259 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 49\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000874 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000285 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Total Bins 178\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 1/12 [00:02<00:27,  2.47s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000202 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000189 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000328 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 42\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000186 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 4\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 2/12 [00:03<00:14,  1.40s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000391 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 127\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000327 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 4\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 3/12 [00:03<00:07,  1.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 0\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000275 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000282 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 38\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000192 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000196 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000259 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 45\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 104\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 140\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 44\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 83\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 4\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 104[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 255\n\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000315 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Info] Total Bins 43\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002839 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 104\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000120 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000073 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 4\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000282 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n\n[LightGBM] [Info] Total Bins 125[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Total Bins 62[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n\n[LightGBM] [Info] Total Bins 116\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000274 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Total Bins 255[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Total Bins 125[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n\n[LightGBM] [Info] Total Bins 48[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 0[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000304 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 4/12 [00:05<00:09,  1.22s/it]","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000186 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000281 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000296 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 104\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000184 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 251\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000201 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 116\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 47\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000239 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000296 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000343 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 89\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000289 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 49\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 5/12 [00:05<00:07,  1.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 38\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 83\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000342 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 234\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000285 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000286 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Total Bins 47\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 104\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000198 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 253\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000198 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Total Bins 86[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000495 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 108\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 125\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000192 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 231\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 46[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 206\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 4\n\n[LightGBM] [Info] Total Bins 219\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000431 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 47\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 45\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000184 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000256 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000320 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000125 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 114\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 163\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000210 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 45[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 7/12 [00:07<00:04,  1.12it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 45\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 8/12 [00:07<00:02,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 253\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000190 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000120 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 253\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 41\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000417 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000676 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000256 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 255[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 254\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 255\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000290 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000289 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 254[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 9/12 [00:08<00:02,  1.42it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000218 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Total Bins 46[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 36\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 104\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 44\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 42\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000259 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 37\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 254\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 42\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000099 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000234 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000259 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 47\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000299 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000305 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 227\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000328 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000313 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000427 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000314 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n\n[LightGBM] [Info] Total Bins 41\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000288 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 10/12 [00:09<00:01,  1.13it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 44\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 46\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12/12 [00:10<00:00,  1.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"83 same features have been deleted.\nMeet early-stopping in successive feature-wise halving.\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/12 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001905 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001998 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001848 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002102 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001440 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001390 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001437 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001431 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001933 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 244\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 138\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001894 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 228\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001346 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001901 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001993 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001946 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001897 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 205\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001908 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001852 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 254\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001909 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 55\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 155\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001879 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002834 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001911 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 114\n\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001389 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 51\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001904 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001875 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001453 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002093 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001954 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001354 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001957 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 58\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001441 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001376 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001443 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001401 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 192\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002004 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 58\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 1/12 [00:04<00:49,  4.54s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001358 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 49\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 2/12 [00:05<00:21,  2.17s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001936 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 56\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001956 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001354 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002001 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 3/12 [00:05<00:11,  1.33s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001951 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 38\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002068 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001992 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 55\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001507 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001935 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 237\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001912 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001906 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 56\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001389 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001345 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001997 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 57\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001916 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001912 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001896 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001350 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 52\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002019 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 57\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002012 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001924 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 253\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001368 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 46\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001383 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 231\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001353 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001875 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001354 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001922 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 42\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002275 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 152\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001381 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001919 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 4/12 [00:11<00:24,  3.05s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001897 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001900 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001900 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 110\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001913 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001905 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002067 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001911 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001898 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 138\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001910 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001911 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 52\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002078 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001912 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 109\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001898 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001910 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001908 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 128\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001903 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001901 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 52\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001910 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002015 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001976 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 5/12 [00:14<00:23,  3.31s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001932 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 56\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002075 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 57\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001398 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 115\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001400 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001912 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 57\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001936 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001909 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 6/12 [00:17<00:18,  3.05s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001930 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 195\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001898 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001899 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001882 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001886 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001920 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 7/12 [00:18<00:11,  2.29s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001435 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 114\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001919 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 55\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001393 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 56\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 127\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001804 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001368 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001414 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001343 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 181\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003697 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Total Bins 238\n\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002106 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001914 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001824 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 58\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001918 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000378 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001471 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001910 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 88\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 8/12 [00:20<00:09,  2.34s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 166\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001898 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001916 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001896 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 137\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 46\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001929 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001899 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001896 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001914 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001417 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001344 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001388 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 143\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001381 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002024 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 39\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001892 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001912 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001899 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001364 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 55\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001906 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 46\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001962 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001952 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001887 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 9/12 [00:24<00:08,  2.86s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001387 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 52\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000382 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001847 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 10/12 [00:25<00:04,  2.41s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 46\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001423 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 57\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001351 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001442 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001885 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 56\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002068 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001989 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 208\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 11/12 [00:28<00:02,  2.35s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002145 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 235\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001442 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001388 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001539 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001353 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001878 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 1\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12/12 [00:31<00:00,  2.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"The number of remaining candidate features is 55\nStart stage II selection.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:07<00:00,  1.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finish data processing.\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039139 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 14200\n[LightGBM] [Info] Number of data points in the train set: 75833, number of used features: 63\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  Sex  Length  Diameter  Height  Whole weight  Whole weight.1  Whole weight.2  Shell weight  \\\n0   F   0.550     0.430   0.150        0.7715          0.3285          0.1465        0.2400   \n1   F   0.630     0.490   0.145        1.1300          0.4580          0.2765        0.3200   \n2   I   0.160     0.110   0.025        0.0210          0.0055          0.0030        0.0050   \n3   M   0.595     0.475   0.150        0.9145          0.3755          0.2055        0.2500   \n4   I   0.555     0.425   0.130        0.7820          0.3695          0.1600        0.1975   \n\n   autoFE_f_0  autoFE_f_1  autoFE_f_2  autoFE_f_3  autoFE_f_4  autoFE_f_5  autoFE_f_6  autoFE_f_7  \\\n0      2672.0    2.348554      0.4430     -0.0935    0.489898      0.2400    0.157544       136.0   \n1      1204.0    2.467249      0.6720     -0.0435    0.565685      0.3200    0.837460        88.0   \n2       499.0    3.818182      0.0155     -0.0020    0.070711      0.0050    0.155689       102.0   \n3      2130.0    2.435419      0.5390     -0.0445    0.500000      0.2500    0.322763       112.0   \n4        34.0    2.116373      0.4125     -0.0375    0.444410      0.1975    0.835659        86.0   \n\n   autoFE_f_8  autoFE_f_9  autoFE_f_10  autoFE_f_11  autoFE_f_12  autoFE_f_13  autoFE_f_14  \\\n0      0.0900      0.3100       0.1900     3.666667       3562.0     3.214583     2.242321   \n1      0.1750      0.3100       0.1700     4.344828       1809.0     3.531250     1.656420   \n2     -0.0200      0.1550       0.1050     6.400000        131.0     4.200000     1.833333   \n3      0.1000      0.3450       0.2250     3.966667       3374.0     3.658000     1.827251   \n4      0.0675      0.3575       0.2275     4.269231       2298.0     3.959494     2.309375   \n\n   autoFE_f_15  autoFE_f_16  autoFE_f_17  autoFE_f_18  autoFE_f_19  autoFE_f_20  autoFE_f_21  \\\n0        601.0       0.4035     0.250692     0.243335      -0.0035     0.610417        340.0   \n1         40.0       0.3535     0.566930     0.956930       0.1315     0.864063         86.0   \n2         65.0       0.1570     0.003299     0.344311      -0.0220     0.600000         58.0   \n3        102.0       0.3895     0.416317     0.349879       0.0555     0.822000        379.0   \n4        588.0       0.3950     0.936966     0.508903       0.0300     0.810127        382.0   \n\n   autoFE_f_22  autoFE_f_23  autoFE_f_24  autoFE_f_25  autoFE_f_26  autoFE_f_27  autoFE_f_28  \\\n0       0.2965     0.278696     5.266212       0.2400       0.5685     2.190000     0.106898   \n1       0.4215     0.743013     4.086799       0.3200       0.7780     3.158621     0.963678   \n2       0.0280     0.006167     7.000000       0.0050       0.0105     0.220000     0.514970   \n3       0.3555     0.618255     4.450122       0.2500       0.6255     2.503333     0.389820   \n4       0.2900     0.919172     4.887500       0.1975       0.5670     2.842308     0.721447   \n\n   autoFE_f_29  autoFE_f_30  autoFE_f_31  autoFE_f_32  autoFE_f_33  autoFE_f_34  autoFE_f_35  \\\n0       0.1785       1.0115       0.5765       0.7715       0.7900     2.935154    -1.920730   \n1       0.3130       1.4500       0.7665       0.1300       0.9500     1.772152    -1.285544   \n2      -0.0195       0.0260       0.1130       0.0210       0.1650    36.666667    -5.809143   \n3       0.2255       1.1645       0.6805       0.9145       0.8450     2.311436    -1.582309   \n4       0.2395       0.9795       0.5850       0.7820       0.7525     2.656250    -1.832581   \n\n   autoFE_f_36  autoFE_f_37  autoFE_f_38  autoFE_f_39  autoFE_f_40  autoFE_f_41  autoFE_f_42  \\\n0       0.7585       1.2015       1.1000       0.6700       1.1375       0.9180       0.7715   \n1       0.9480       1.6200       1.5880       0.8100       0.8760       1.4065       1.1300   \n2       0.1155       0.1310       0.0265       0.1150       0.0240       0.0240       0.1600   \n3       0.8505       1.3895       1.2900       0.7250       1.1375       1.1200       0.9145   \n4       0.7945       1.2070       1.1515       0.6225       0.8175       0.9420       0.7820   \n\n   autoFE_f_43  autoFE_f_44  autoFE_f_45  autoFE_f_46  autoFE_f_47  autoFE_f_48  autoFE_f_49  \\\n0       2.1400     0.253438       0.4405       0.1465        0.700       0.2400        0.575   \n1       1.8075     0.517540       0.4695       0.1450        0.775       0.3200        0.585   \n2       0.0545     0.000116       0.0125       0.0030        0.185       0.0050        0.185   \n3       2.1400     0.343395       0.4405       0.1500        0.745       0.2500        0.575   \n4       1.5560     0.288949       0.3950       0.1300        0.685       0.1975        0.585   \n\n   autoFE_f_50  autoFE_f_51  autoFE_f_52  autoFE_f_53  autoFE_f_54  Rings  \n0       0.2400       0.1465     0.062995     0.283764       0.1465     11  \n1       0.3200       0.2765     0.135485     0.627387       0.2765     11  \n2       0.0050       0.0030     0.000330     0.006131       0.0030      6  \n3       0.2500       0.2055     0.097612     0.397943       0.2055     10  \n4       0.1975       0.1600     0.068000     0.857709       0.1600      9  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>Length</th>\n      <th>Diameter</th>\n      <th>Height</th>\n      <th>Whole weight</th>\n      <th>Whole weight.1</th>\n      <th>Whole weight.2</th>\n      <th>Shell weight</th>\n      <th>autoFE_f_0</th>\n      <th>autoFE_f_1</th>\n      <th>autoFE_f_2</th>\n      <th>autoFE_f_3</th>\n      <th>autoFE_f_4</th>\n      <th>autoFE_f_5</th>\n      <th>autoFE_f_6</th>\n      <th>autoFE_f_7</th>\n      <th>autoFE_f_8</th>\n      <th>autoFE_f_9</th>\n      <th>autoFE_f_10</th>\n      <th>autoFE_f_11</th>\n      <th>autoFE_f_12</th>\n      <th>autoFE_f_13</th>\n      <th>autoFE_f_14</th>\n      <th>autoFE_f_15</th>\n      <th>autoFE_f_16</th>\n      <th>autoFE_f_17</th>\n      <th>autoFE_f_18</th>\n      <th>autoFE_f_19</th>\n      <th>autoFE_f_20</th>\n      <th>autoFE_f_21</th>\n      <th>autoFE_f_22</th>\n      <th>autoFE_f_23</th>\n      <th>autoFE_f_24</th>\n      <th>autoFE_f_25</th>\n      <th>autoFE_f_26</th>\n      <th>autoFE_f_27</th>\n      <th>autoFE_f_28</th>\n      <th>autoFE_f_29</th>\n      <th>autoFE_f_30</th>\n      <th>autoFE_f_31</th>\n      <th>autoFE_f_32</th>\n      <th>autoFE_f_33</th>\n      <th>autoFE_f_34</th>\n      <th>autoFE_f_35</th>\n      <th>autoFE_f_36</th>\n      <th>autoFE_f_37</th>\n      <th>autoFE_f_38</th>\n      <th>autoFE_f_39</th>\n      <th>autoFE_f_40</th>\n      <th>autoFE_f_41</th>\n      <th>autoFE_f_42</th>\n      <th>autoFE_f_43</th>\n      <th>autoFE_f_44</th>\n      <th>autoFE_f_45</th>\n      <th>autoFE_f_46</th>\n      <th>autoFE_f_47</th>\n      <th>autoFE_f_48</th>\n      <th>autoFE_f_49</th>\n      <th>autoFE_f_50</th>\n      <th>autoFE_f_51</th>\n      <th>autoFE_f_52</th>\n      <th>autoFE_f_53</th>\n      <th>autoFE_f_54</th>\n      <th>Rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>F</td>\n      <td>0.550</td>\n      <td>0.430</td>\n      <td>0.150</td>\n      <td>0.7715</td>\n      <td>0.3285</td>\n      <td>0.1465</td>\n      <td>0.2400</td>\n      <td>2672.0</td>\n      <td>2.348554</td>\n      <td>0.4430</td>\n      <td>-0.0935</td>\n      <td>0.489898</td>\n      <td>0.2400</td>\n      <td>0.157544</td>\n      <td>136.0</td>\n      <td>0.0900</td>\n      <td>0.3100</td>\n      <td>0.1900</td>\n      <td>3.666667</td>\n      <td>3562.0</td>\n      <td>3.214583</td>\n      <td>2.242321</td>\n      <td>601.0</td>\n      <td>0.4035</td>\n      <td>0.250692</td>\n      <td>0.243335</td>\n      <td>-0.0035</td>\n      <td>0.610417</td>\n      <td>340.0</td>\n      <td>0.2965</td>\n      <td>0.278696</td>\n      <td>5.266212</td>\n      <td>0.2400</td>\n      <td>0.5685</td>\n      <td>2.190000</td>\n      <td>0.106898</td>\n      <td>0.1785</td>\n      <td>1.0115</td>\n      <td>0.5765</td>\n      <td>0.7715</td>\n      <td>0.7900</td>\n      <td>2.935154</td>\n      <td>-1.920730</td>\n      <td>0.7585</td>\n      <td>1.2015</td>\n      <td>1.1000</td>\n      <td>0.6700</td>\n      <td>1.1375</td>\n      <td>0.9180</td>\n      <td>0.7715</td>\n      <td>2.1400</td>\n      <td>0.253438</td>\n      <td>0.4405</td>\n      <td>0.1465</td>\n      <td>0.700</td>\n      <td>0.2400</td>\n      <td>0.575</td>\n      <td>0.2400</td>\n      <td>0.1465</td>\n      <td>0.062995</td>\n      <td>0.283764</td>\n      <td>0.1465</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>F</td>\n      <td>0.630</td>\n      <td>0.490</td>\n      <td>0.145</td>\n      <td>1.1300</td>\n      <td>0.4580</td>\n      <td>0.2765</td>\n      <td>0.3200</td>\n      <td>1204.0</td>\n      <td>2.467249</td>\n      <td>0.6720</td>\n      <td>-0.0435</td>\n      <td>0.565685</td>\n      <td>0.3200</td>\n      <td>0.837460</td>\n      <td>88.0</td>\n      <td>0.1750</td>\n      <td>0.3100</td>\n      <td>0.1700</td>\n      <td>4.344828</td>\n      <td>1809.0</td>\n      <td>3.531250</td>\n      <td>1.656420</td>\n      <td>40.0</td>\n      <td>0.3535</td>\n      <td>0.566930</td>\n      <td>0.956930</td>\n      <td>0.1315</td>\n      <td>0.864063</td>\n      <td>86.0</td>\n      <td>0.4215</td>\n      <td>0.743013</td>\n      <td>4.086799</td>\n      <td>0.3200</td>\n      <td>0.7780</td>\n      <td>3.158621</td>\n      <td>0.963678</td>\n      <td>0.3130</td>\n      <td>1.4500</td>\n      <td>0.7665</td>\n      <td>0.1300</td>\n      <td>0.9500</td>\n      <td>1.772152</td>\n      <td>-1.285544</td>\n      <td>0.9480</td>\n      <td>1.6200</td>\n      <td>1.5880</td>\n      <td>0.8100</td>\n      <td>0.8760</td>\n      <td>1.4065</td>\n      <td>1.1300</td>\n      <td>1.8075</td>\n      <td>0.517540</td>\n      <td>0.4695</td>\n      <td>0.1450</td>\n      <td>0.775</td>\n      <td>0.3200</td>\n      <td>0.585</td>\n      <td>0.3200</td>\n      <td>0.2765</td>\n      <td>0.135485</td>\n      <td>0.627387</td>\n      <td>0.2765</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I</td>\n      <td>0.160</td>\n      <td>0.110</td>\n      <td>0.025</td>\n      <td>0.0210</td>\n      <td>0.0055</td>\n      <td>0.0030</td>\n      <td>0.0050</td>\n      <td>499.0</td>\n      <td>3.818182</td>\n      <td>0.0155</td>\n      <td>-0.0020</td>\n      <td>0.070711</td>\n      <td>0.0050</td>\n      <td>0.155689</td>\n      <td>102.0</td>\n      <td>-0.0200</td>\n      <td>0.1550</td>\n      <td>0.1050</td>\n      <td>6.400000</td>\n      <td>131.0</td>\n      <td>4.200000</td>\n      <td>1.833333</td>\n      <td>65.0</td>\n      <td>0.1570</td>\n      <td>0.003299</td>\n      <td>0.344311</td>\n      <td>-0.0220</td>\n      <td>0.600000</td>\n      <td>58.0</td>\n      <td>0.0280</td>\n      <td>0.006167</td>\n      <td>7.000000</td>\n      <td>0.0050</td>\n      <td>0.0105</td>\n      <td>0.220000</td>\n      <td>0.514970</td>\n      <td>-0.0195</td>\n      <td>0.0260</td>\n      <td>0.1130</td>\n      <td>0.0210</td>\n      <td>0.1650</td>\n      <td>36.666667</td>\n      <td>-5.809143</td>\n      <td>0.1155</td>\n      <td>0.1310</td>\n      <td>0.0265</td>\n      <td>0.1150</td>\n      <td>0.0240</td>\n      <td>0.0240</td>\n      <td>0.1600</td>\n      <td>0.0545</td>\n      <td>0.000116</td>\n      <td>0.0125</td>\n      <td>0.0030</td>\n      <td>0.185</td>\n      <td>0.0050</td>\n      <td>0.185</td>\n      <td>0.0050</td>\n      <td>0.0030</td>\n      <td>0.000330</td>\n      <td>0.006131</td>\n      <td>0.0030</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>M</td>\n      <td>0.595</td>\n      <td>0.475</td>\n      <td>0.150</td>\n      <td>0.9145</td>\n      <td>0.3755</td>\n      <td>0.2055</td>\n      <td>0.2500</td>\n      <td>2130.0</td>\n      <td>2.435419</td>\n      <td>0.5390</td>\n      <td>-0.0445</td>\n      <td>0.500000</td>\n      <td>0.2500</td>\n      <td>0.322763</td>\n      <td>112.0</td>\n      <td>0.1000</td>\n      <td>0.3450</td>\n      <td>0.2250</td>\n      <td>3.966667</td>\n      <td>3374.0</td>\n      <td>3.658000</td>\n      <td>1.827251</td>\n      <td>102.0</td>\n      <td>0.3895</td>\n      <td>0.416317</td>\n      <td>0.349879</td>\n      <td>0.0555</td>\n      <td>0.822000</td>\n      <td>379.0</td>\n      <td>0.3555</td>\n      <td>0.618255</td>\n      <td>4.450122</td>\n      <td>0.2500</td>\n      <td>0.6255</td>\n      <td>2.503333</td>\n      <td>0.389820</td>\n      <td>0.2255</td>\n      <td>1.1645</td>\n      <td>0.6805</td>\n      <td>0.9145</td>\n      <td>0.8450</td>\n      <td>2.311436</td>\n      <td>-1.582309</td>\n      <td>0.8505</td>\n      <td>1.3895</td>\n      <td>1.2900</td>\n      <td>0.7250</td>\n      <td>1.1375</td>\n      <td>1.1200</td>\n      <td>0.9145</td>\n      <td>2.1400</td>\n      <td>0.343395</td>\n      <td>0.4405</td>\n      <td>0.1500</td>\n      <td>0.745</td>\n      <td>0.2500</td>\n      <td>0.575</td>\n      <td>0.2500</td>\n      <td>0.2055</td>\n      <td>0.097612</td>\n      <td>0.397943</td>\n      <td>0.2055</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I</td>\n      <td>0.555</td>\n      <td>0.425</td>\n      <td>0.130</td>\n      <td>0.7820</td>\n      <td>0.3695</td>\n      <td>0.1600</td>\n      <td>0.1975</td>\n      <td>34.0</td>\n      <td>2.116373</td>\n      <td>0.4125</td>\n      <td>-0.0375</td>\n      <td>0.444410</td>\n      <td>0.1975</td>\n      <td>0.835659</td>\n      <td>86.0</td>\n      <td>0.0675</td>\n      <td>0.3575</td>\n      <td>0.2275</td>\n      <td>4.269231</td>\n      <td>2298.0</td>\n      <td>3.959494</td>\n      <td>2.309375</td>\n      <td>588.0</td>\n      <td>0.3950</td>\n      <td>0.936966</td>\n      <td>0.508903</td>\n      <td>0.0300</td>\n      <td>0.810127</td>\n      <td>382.0</td>\n      <td>0.2900</td>\n      <td>0.919172</td>\n      <td>4.887500</td>\n      <td>0.1975</td>\n      <td>0.5670</td>\n      <td>2.842308</td>\n      <td>0.721447</td>\n      <td>0.2395</td>\n      <td>0.9795</td>\n      <td>0.5850</td>\n      <td>0.7820</td>\n      <td>0.7525</td>\n      <td>2.656250</td>\n      <td>-1.832581</td>\n      <td>0.7945</td>\n      <td>1.2070</td>\n      <td>1.1515</td>\n      <td>0.6225</td>\n      <td>0.8175</td>\n      <td>0.9420</td>\n      <td>0.7820</td>\n      <td>1.5560</td>\n      <td>0.288949</td>\n      <td>0.3950</td>\n      <td>0.1300</td>\n      <td>0.685</td>\n      <td>0.1975</td>\n      <td>0.585</td>\n      <td>0.1975</td>\n      <td>0.1600</td>\n      <td>0.068000</td>\n      <td>0.857709</td>\n      <td>0.1600</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:01:31.968119Z","iopub.execute_input":"2024-05-18T10:01:31.969185Z","iopub.status.idle":"2024-05-18T10:01:31.976813Z","shell.execute_reply.started":"2024-05-18T10:01:31.969145Z","shell.execute_reply":"2024-05-18T10:01:31.975667Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(94792, 64)"},"metadata":{}}]},{"cell_type":"code","source":"train.Rings = np.log1p(train.Rings)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:02:23.453661Z","iopub.execute_input":"2024-05-18T10:02:23.454024Z","iopub.status.idle":"2024-05-18T10:02:23.459984Z","shell.execute_reply.started":"2024-05-18T10:02:23.453995Z","shell.execute_reply":"2024-05-18T10:02:23.458878Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\ndef rmsle(y_true,y_pred):\n    rmsle = np.sqrt(mean_squared_log_error(y_true,y_pred))\n    return rmsle\n\nfrom autogluon.core.metrics import make_scorer\nag_rmsle_scorer = make_scorer(name='mean_squared_log_error',score_func=rmsle,optimum=0,greater_is_better=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:02:29.733806Z","iopub.execute_input":"2024-05-18T10:02:29.734179Z","iopub.status.idle":"2024-05-18T10:02:29.756135Z","shell.execute_reply.started":"2024-05-18T10:02:29.734152Z","shell.execute_reply":"2024-05-18T10:02:29.755394Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from autogluon.tabular import TabularDataset, TabularPredictor\n\ntime_limit = 3600*8\n\nautoml = TabularPredictor(label='Rings',problem_type='regression',eval_metric=ag_rmsle_scorer)\n\nautoml.fit(train,presets='best_quality',time_limit=time_limit,\n           num_bag_folds=8,num_bag_sets=0,num_stack_levels=1,\n          dynamic_stacking=False,\n          included_model_types=['XGB','CAT','GBM','XT'],\n          ag_args_fit={\n              'num_gpus':1,\n              'num_cpus':4\n          })","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:03:40.740171Z","iopub.execute_input":"2024-05-18T10:03:40.740565Z","iopub.status.idle":"2024-05-18T12:31:59.999314Z","shell.execute_reply.started":"2024-05-18T10:03:40.740536Z","shell.execute_reply":"2024-05-18T12:31:59.998550Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20240518_100340\"\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=0\nBeginning AutoGluon training ... Time limit = 28800s\nAutoGluon will save models to \"AutogluonModels/ag-20240518_100340\"\n=================== System Info ===================\nAutoGluon Version:  1.1.0\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Tue Dec 19 13:14:11 UTC 2023\nCPU Count:          4\nMemory Avail:       29.85 GB / 31.36 GB (95.2%)\nDisk Space Avail:   19.45 GB / 19.52 GB (99.7%)\n===================================================\nTrain Data Rows:    94792\nTrain Data Columns: 63\nLabel Column:       Rings\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30624.45 MB\n\tTrain Data (Original)  Memory Usage: 50.08 MB (0.2% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tUnused Original Features (Count: 2): ['autoFE_f_48', 'autoFE_f_50']\n\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\t\tThese features do not need to be present at inference time.\n\t\t('float', []) : 2 | ['autoFE_f_48', 'autoFE_f_50']\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', [])  : 60 | ['Length', 'Diameter', 'Height', 'Whole weight', 'Whole weight.1', ...]\n\t\t('object', []) :  1 | ['Sex']\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', []) :  1 | ['Sex']\n\t\t('float', [])    : 60 | ['Length', 'Diameter', 'Height', 'Whole weight', 'Whole weight.1', ...]\n\t0.9s = Fit runtime\n\t61 features in original data used to generate 61 features in processed data.\n\tTrain Data (Processed) Memory Usage: 43.48 MB (0.1% of available memory)\nData preprocessing and feature engineering runtime = 1.01s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_log_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nLarge model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nIncluded models: ['XGB', 'CAT', 'GBM', 'XT'] (Specified by `included_model_types`, all other model types will be skipped)\nFitting 54 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 19194.53s of the 28798.98s of remaining time.\n2024-05-18 10:03:42,620\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\nWill use sequential fold fitting strategy because import of ray failed. Reason: ray==2.9.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` \n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0218991\tvalid_set's mean_squared_log_error: -0.106655\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0447\t = Validation score   (-mean_squared_log_error)\n\t34.44s\t = Training   runtime\n\t0.71s\t = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 19158.31s of the 28762.76s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t21.15s\t = Training   runtime\n\t0.39s\t = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 19136.48s of the 28740.94s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t179.38s\t = Training   runtime\n\t0.32s\t = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 18956.5s of the 28560.95s of remaining time.\n\t-0.045\t = Validation score   (-mean_squared_log_error)\n\t100.52s\t = Training   runtime\n\t5.61s\t = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 18849.14s of the 28453.59s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t20.66s\t = Training   runtime\n\t0.46s\t = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 18827.72s of the 28432.17s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t68.81s\t = Training   runtime\n\t0.72s\t = Validation runtime\nFitting model: CatBoost_r177_BAG_L1 ... Training model for up to 18757.49s of the 28361.94s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t63.7s\t = Training   runtime\n\t0.33s\t = Validation runtime\nFitting model: LightGBM_r131_BAG_L1 ... Training model for up to 18693.17s of the 28297.62s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t70.54s\t = Training   runtime\n\t1.38s\t = Validation runtime\nFitting model: CatBoost_r9_BAG_L1 ... Training model for up to 18620.44s of the 28224.9s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t90.1s\t = Training   runtime\n\t1.17s\t = Validation runtime\nFitting model: LightGBM_r96_BAG_L1 ... Training model for up to 18528.83s of the 28133.28s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0220139\tvalid_set's mean_squared_log_error: -0.106765\n[2000]\tvalid_set's l2: 0.0218322\tvalid_set's mean_squared_log_error: -0.106633\n[3000]\tvalid_set's l2: 0.0217384\tvalid_set's mean_squared_log_error: -0.106654\n[4000]\tvalid_set's l2: 0.0216892\tvalid_set's mean_squared_log_error: -0.106627\n[5000]\tvalid_set's l2: 0.0216655\tvalid_set's mean_squared_log_error: -0.106518\n[6000]\tvalid_set's l2: 0.0216423\tvalid_set's mean_squared_log_error: -0.106536\n[7000]\tvalid_set's l2: 0.0216386\tvalid_set's mean_squared_log_error: -0.106442\n[8000]\tvalid_set's l2: 0.0216337\tvalid_set's mean_squared_log_error: -0.10642\n[9000]\tvalid_set's l2: 0.0216421\tvalid_set's mean_squared_log_error: -0.106445\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0231787\tvalid_set's mean_squared_log_error: -0.106681\n[2000]\tvalid_set's l2: 0.0229956\tvalid_set's mean_squared_log_error: -0.106555\n[3000]\tvalid_set's l2: 0.0229097\tvalid_set's mean_squared_log_error: -0.106532\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0219093\tvalid_set's mean_squared_log_error: -0.107133\n[2000]\tvalid_set's l2: 0.0217535\tvalid_set's mean_squared_log_error: -0.106996\n[3000]\tvalid_set's l2: 0.0216827\tvalid_set's mean_squared_log_error: -0.106908\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.021629\tvalid_set's mean_squared_log_error: -0.106596\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0221786\tvalid_set's mean_squared_log_error: -0.105946\n[2000]\tvalid_set's l2: 0.0219797\tvalid_set's mean_squared_log_error: -0.105841\n[3000]\tvalid_set's l2: 0.0218717\tvalid_set's mean_squared_log_error: -0.105779\n","output_type":"stream"},{"name":"stderr","text":"\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t129.95s\t = Training   runtime\n\t5.65s\t = Validation runtime\nFitting model: XGBoost_r33_BAG_L1 ... Training model for up to 18392.09s of the 27996.54s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t97.07s\t = Training   runtime\n\t0.6s\t = Validation runtime\nFitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 18293.73s of the 27898.19s of remaining time.\n\t-0.045\t = Validation score   (-mean_squared_log_error)\n\t73.88s\t = Training   runtime\n\t6.34s\t = Validation runtime\nFitting model: CatBoost_r137_BAG_L1 ... Training model for up to 18212.22s of the 27816.67s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t90.25s\t = Training   runtime\n\t0.39s\t = Validation runtime\nFitting model: CatBoost_r13_BAG_L1 ... Training model for up to 18121.3s of the 27725.75s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t150.04s\t = Training   runtime\n\t0.48s\t = Validation runtime\nFitting model: LightGBM_r188_BAG_L1 ... Training model for up to 17970.26s of the 27574.71s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t65.86s\t = Training   runtime\n\t1.02s\t = Validation runtime\nFitting model: XGBoost_r89_BAG_L1 ... Training model for up to 17902.53s of the 27506.98s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t25.93s\t = Training   runtime\n\t0.51s\t = Validation runtime\nFitting model: LightGBM_r130_BAG_L1 ... Training model for up to 17875.79s of the 27480.24s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t41.73s\t = Training   runtime\n\t0.62s\t = Validation runtime\nFitting model: CatBoost_r50_BAG_L1 ... Training model for up to 17832.94s of the 27437.39s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t78.88s\t = Training   runtime\n\t0.97s\t = Validation runtime\nFitting model: XGBoost_r194_BAG_L1 ... Training model for up to 17752.8s of the 27357.25s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t14.87s\t = Training   runtime\n\t0.25s\t = Validation runtime\nFitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 17737.39s of the 27341.84s of remaining time.\n\t-0.0447\t = Validation score   (-mean_squared_log_error)\n\t64.89s\t = Training   runtime\n\t5.73s\t = Validation runtime\nFitting model: CatBoost_r69_BAG_L1 ... Training model for up to 17665.83s of the 27270.28s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t170.17s\t = Training   runtime\n\t0.36s\t = Validation runtime\nFitting model: LightGBM_r161_BAG_L1 ... Training model for up to 17495.03s of the 27099.48s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0227814\tvalid_set's mean_squared_log_error: -0.106396\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t266.75s\t = Training   runtime\n\t3.99s\t = Validation runtime\nFitting model: CatBoost_r70_BAG_L1 ... Training model for up to 17221.09s of the 26825.54s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t98.07s\t = Training   runtime\n\t1.3s\t = Validation runtime\nFitting model: LightGBM_r196_BAG_L1 ... Training model for up to 17121.42s of the 26725.87s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0868\t = Validation score   (-mean_squared_log_error)\n\t13.61s\t = Training   runtime\n\t0.11s\t = Validation runtime\nFitting model: CatBoost_r167_BAG_L1 ... Training model for up to 17107.34s of the 26711.79s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t99.54s\t = Training   runtime\n\t0.29s\t = Validation runtime\nFitting model: XGBoost_r98_BAG_L1 ... Training model for up to 17007.23s of the 26611.68s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t207.34s\t = Training   runtime\n\t0.58s\t = Validation runtime\nFitting model: LightGBM_r15_BAG_L1 ... Training model for up to 16797.85s of the 26402.3s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t37.53s\t = Training   runtime\n\t0.85s\t = Validation runtime\nFitting model: CatBoost_r86_BAG_L1 ... Training model for up to 16759.1s of the 26363.56s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t212.23s\t = Training   runtime\n\t0.39s\t = Validation runtime\nFitting model: CatBoost_r49_BAG_L1 ... Training model for up to 16546.13s of the 26150.58s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t71.59s\t = Training   runtime\n\t0.34s\t = Validation runtime\nFitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 16473.82s of the 26078.27s of remaining time.\n\t-0.0451\t = Validation score   (-mean_squared_log_error)\n\t19.77s\t = Training   runtime\n\t6.57s\t = Validation runtime\nFitting model: LightGBM_r143_BAG_L1 ... Training model for up to 16445.47s of the 26049.92s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0216781\tvalid_set's mean_squared_log_error: -0.106049\n","output_type":"stream"},{"name":"stderr","text":"\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t195.57s\t = Training   runtime\n\t2.79s\t = Validation runtime\nFitting model: LightGBM_r94_BAG_L1 ... Training model for up to 16245.2s of the 25849.65s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.022104\tvalid_set's mean_squared_log_error: -0.106507\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0230088\tvalid_set's mean_squared_log_error: -0.106516\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0219782\tvalid_set's mean_squared_log_error: -0.105717\n","output_type":"stream"},{"name":"stderr","text":"\t-0.0447\t = Validation score   (-mean_squared_log_error)\n\t40.48s\t = Training   runtime\n\t1.4s\t = Validation runtime\nFitting model: CatBoost_r128_BAG_L1 ... Training model for up to 16202.9s of the 25807.35s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t72.07s\t = Training   runtime\n\t0.92s\t = Validation runtime\nFitting model: ExtraTrees_r4_BAG_L1 ... Training model for up to 16129.59s of the 25734.04s of remaining time.\n\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t42.6s\t = Training   runtime\n\t4.29s\t = Validation runtime\nFitting model: LightGBM_r30_BAG_L1 ... Training model for up to 16082.35s of the 25686.8s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0219593\tvalid_set's mean_squared_log_error: -0.106542\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0218383\tvalid_set's mean_squared_log_error: -0.1059\n","output_type":"stream"},{"name":"stderr","text":"\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t127.49s\t = Training   runtime\n\t3.01s\t = Validation runtime\nFitting model: XGBoost_r49_BAG_L1 ... Training model for up to 15950.45s of the 25554.9s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t35.12s\t = Training   runtime\n\t0.51s\t = Validation runtime\nFitting model: CatBoost_r5_BAG_L1 ... Training model for up to 15914.48s of the 25518.93s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t78.96s\t = Training   runtime\n\t0.36s\t = Validation runtime\nFitting model: CatBoost_r143_BAG_L1 ... Training model for up to 15834.89s of the 25439.34s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t61.05s\t = Training   runtime\n\t0.31s\t = Validation runtime\nFitting model: ExtraTrees_r178_BAG_L1 ... Training model for up to 15773.14s of the 25377.59s of remaining time.\n\t-0.0447\t = Validation score   (-mean_squared_log_error)\n\t44.08s\t = Training   runtime\n\t4.96s\t = Validation runtime\nFitting model: XGBoost_r31_BAG_L1 ... Training model for up to 15723.17s of the 25327.62s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t232.91s\t = Training   runtime\n\t1.08s\t = Validation runtime\nFitting model: CatBoost_r60_BAG_L1 ... Training model for up to 15488.09s of the 25092.54s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t82.4s\t = Training   runtime\n\t0.32s\t = Validation runtime\nFitting model: LightGBM_r135_BAG_L1 ... Training model for up to 15405.08s of the 25009.53s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t84.22s\t = Training   runtime\n\t1.02s\t = Validation runtime\nFitting model: XGBoost_r22_BAG_L1 ... Training model for up to 15318.81s of the 24923.26s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t21.73s\t = Training   runtime\n\t0.47s\t = Validation runtime\nFitting model: CatBoost_r6_BAG_L1 ... Training model for up to 15296.31s of the 24900.76s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t68.96s\t = Training   runtime\n\t0.86s\t = Validation runtime\nFitting model: LightGBM_r121_BAG_L1 ... Training model for up to 15226.22s of the 24830.67s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0216039\tvalid_set's mean_squared_log_error: -0.105971\n","output_type":"stream"},{"name":"stderr","text":"\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t209.51s\t = Training   runtime\n\t3.23s\t = Validation runtime\nFitting model: CatBoost_r180_BAG_L1 ... Training model for up to 15011.24s of the 24615.69s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t58.75s\t = Training   runtime\n\t0.87s\t = Validation runtime\nFitting model: ExtraTrees_r197_BAG_L1 ... Training model for up to 14951.34s of the 24555.79s of remaining time.\n\t-0.0451\t = Validation score   (-mean_squared_log_error)\n\t99.8s\t = Training   runtime\n\t6.79s\t = Validation runtime\nFitting model: CatBoost_r12_BAG_L1 ... Training model for up to 14842.13s of the 24446.59s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t100.32s\t = Training   runtime\n\t0.35s\t = Validation runtime\nFitting model: ExtraTrees_r126_BAG_L1 ... Training model for up to 14741.16s of the 24345.61s of remaining time.\n\t-0.045\t = Validation score   (-mean_squared_log_error)\n\t14.9s\t = Training   runtime\n\t5.85s\t = Validation runtime\nFitting model: CatBoost_r163_BAG_L1 ... Training model for up to 14718.62s of the 24323.08s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t75.82s\t = Training   runtime\n\t0.27s\t = Validation runtime\nFitting model: CatBoost_r198_BAG_L1 ... Training model for up to 14642.26s of the 24246.71s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t81.0s\t = Training   runtime\n\t0.34s\t = Validation runtime\nFitting model: XGBoost_r95_BAG_L1 ... Training model for up to 14560.51s of the 24164.96s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t28.33s\t = Training   runtime\n\t0.53s\t = Validation runtime\nFitting model: XGBoost_r34_BAG_L1 ... Training model for up to 14531.22s of the 24135.67s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t57.79s\t = Training   runtime\n\t0.53s\t = Validation runtime\nFitting model: LightGBM_r42_BAG_L1 ... Training model for up to 14472.41s of the 24076.87s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t58.38s\t = Training   runtime\n\t0.84s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 1919.45s of the 24016.78s of remaining time.\n\tEnsemble Weights: {'XGBoost_r31_BAG_L1': 0.188, 'LightGBM_r130_BAG_L1': 0.125, 'LightGBM_r161_BAG_L1': 0.125, 'XGBoost_r98_BAG_L1': 0.125, 'LightGBM_r121_BAG_L1': 0.125, 'CatBoost_r9_BAG_L1': 0.062, 'CatBoost_r13_BAG_L1': 0.062, 'ExtraTrees_r172_BAG_L1': 0.062, 'ExtraTrees_r178_BAG_L1': 0.062, 'LightGBM_r135_BAG_L1': 0.062}\n\t-0.0442\t = Validation score   (-mean_squared_log_error)\n\t0.97s\t = Training   runtime\n\t0.0s\t = Validation runtime\nIncluded models: ['XGB', 'CAT', 'GBM', 'XT'] (Specified by `included_model_types`, all other model types will be skipped)\nFitting 54 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 24015.77s of the 24015.6s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t26.0s\t = Training   runtime\n\t0.37s\t = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 23988.7s of the 23988.53s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t27.37s\t = Training   runtime\n\t0.33s\t = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 23960.25s of the 23960.08s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t48.03s\t = Training   runtime\n\t0.35s\t = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 23911.18s of the 23911.01s of remaining time.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t209.84s\t = Training   runtime\n\t7.53s\t = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 23692.4s of the 23692.24s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t24.65s\t = Training   runtime\n\t0.81s\t = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 23666.28s of the 23666.11s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t92.99s\t = Training   runtime\n\t0.72s\t = Validation runtime\nFitting model: CatBoost_r177_BAG_L2 ... Training model for up to 23571.43s of the 23571.27s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t27.61s\t = Training   runtime\n\t0.32s\t = Validation runtime\nFitting model: LightGBM_r131_BAG_L2 ... Training model for up to 23542.82s of the 23542.65s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0210453\tvalid_set's mean_squared_log_error: -0.105792\n","output_type":"stream"},{"name":"stderr","text":"\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t99.35s\t = Training   runtime\n\t1.65s\t = Validation runtime\nFitting model: CatBoost_r9_BAG_L2 ... Training model for up to 23440.42s of the 23440.26s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t48.81s\t = Training   runtime\n\t0.68s\t = Validation runtime\nFitting model: LightGBM_r96_BAG_L2 ... Training model for up to 23390.24s of the 23390.08s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0221392\tvalid_set's mean_squared_log_error: -0.106769\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t43.41s\t = Training   runtime\n\t0.97s\t = Validation runtime\nFitting model: XGBoost_r33_BAG_L2 ... Training model for up to 23344.93s of the 23344.77s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t100.5s\t = Training   runtime\n\t1.18s\t = Validation runtime\nFitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 23242.35s of the 23242.19s of remaining time.\n\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t165.22s\t = Training   runtime\n\t7.79s\t = Validation runtime\nFitting model: CatBoost_r137_BAG_L2 ... Training model for up to 23067.81s of the 23067.65s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t30.52s\t = Training   runtime\n\t0.34s\t = Validation runtime\nFitting model: CatBoost_r13_BAG_L2 ... Training model for up to 23036.28s of the 23036.11s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t50.87s\t = Training   runtime\n\t0.37s\t = Validation runtime\nFitting model: LightGBM_r188_BAG_L2 ... Training model for up to 22984.15s of the 22983.98s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t60.6s\t = Training   runtime\n\t0.68s\t = Validation runtime\nFitting model: XGBoost_r89_BAG_L2 ... Training model for up to 22921.7s of the 22921.54s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t23.68s\t = Training   runtime\n\t0.81s\t = Validation runtime\nFitting model: LightGBM_r130_BAG_L2 ... Training model for up to 22896.54s of the 22896.38s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t40.55s\t = Training   runtime\n\t0.44s\t = Validation runtime\nFitting model: CatBoost_r50_BAG_L2 ... Training model for up to 22854.77s of the 22854.6s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t28.19s\t = Training   runtime\n\t0.37s\t = Validation runtime\nFitting model: XGBoost_r194_BAG_L2 ... Training model for up to 22825.54s of the 22825.37s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t16.43s\t = Training   runtime\n\t0.39s\t = Validation runtime\nFitting model: ExtraTrees_r172_BAG_L2 ... Training model for up to 22807.96s of the 22807.8s of remaining time.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t156.21s\t = Training   runtime\n\t7.05s\t = Validation runtime\nFitting model: CatBoost_r69_BAG_L2 ... Training model for up to 22643.49s of the 22643.33s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t44.04s\t = Training   runtime\n\t0.35s\t = Validation runtime\nFitting model: LightGBM_r161_BAG_L2 ... Training model for up to 22598.41s of the 22598.24s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0222798\tvalid_set's mean_squared_log_error: -0.10671\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t340.08s\t = Training   runtime\n\t3.79s\t = Validation runtime\nFitting model: CatBoost_r70_BAG_L2 ... Training model for up to 22250.96s of the 22250.8s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t40.37s\t = Training   runtime\n\t0.54s\t = Validation runtime\nFitting model: LightGBM_r196_BAG_L2 ... Training model for up to 22209.37s of the 22209.2s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0868\t = Validation score   (-mean_squared_log_error)\n\t20.13s\t = Training   runtime\n\t0.16s\t = Validation runtime\nFitting model: CatBoost_r167_BAG_L2 ... Training model for up to 22188.25s of the 22188.09s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t37.83s\t = Training   runtime\n\t0.3s\t = Validation runtime\nFitting model: XGBoost_r98_BAG_L2 ... Training model for up to 22149.41s of the 22149.24s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t146.1s\t = Training   runtime\n\t0.71s\t = Validation runtime\nFitting model: LightGBM_r15_BAG_L2 ... Training model for up to 22001.31s of the 22001.14s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t39.37s\t = Training   runtime\n\t0.57s\t = Validation runtime\nFitting model: CatBoost_r86_BAG_L2 ... Training model for up to 21960.65s of the 21960.49s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t58.87s\t = Training   runtime\n\t0.34s\t = Validation runtime\nFitting model: CatBoost_r49_BAG_L2 ... Training model for up to 21900.76s of the 21900.59s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t28.08s\t = Training   runtime\n\t0.3s\t = Validation runtime\nFitting model: ExtraTrees_r49_BAG_L2 ... Training model for up to 21871.51s of the 21871.34s of remaining time.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t28.5s\t = Training   runtime\n\t8.21s\t = Validation runtime\nFitting model: LightGBM_r143_BAG_L2 ... Training model for up to 21832.39s of the 21832.23s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t203.07s\t = Training   runtime\n\t2.22s\t = Validation runtime\nFitting model: LightGBM_r94_BAG_L2 ... Training model for up to 21625.21s of the 21625.05s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t25.47s\t = Training   runtime\n\t0.42s\t = Validation runtime\nFitting model: CatBoost_r128_BAG_L2 ... Training model for up to 21598.64s of the 21598.48s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t45.48s\t = Training   runtime\n\t0.63s\t = Validation runtime\nFitting model: ExtraTrees_r4_BAG_L2 ... Training model for up to 21551.85s of the 21551.68s of remaining time.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t104.39s\t = Training   runtime\n\t6.17s\t = Validation runtime\nFitting model: LightGBM_r30_BAG_L2 ... Training model for up to 21440.74s of the 21440.57s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0209075\tvalid_set's mean_squared_log_error: -0.10618\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t148.96s\t = Training   runtime\n\t2.83s\t = Validation runtime\nFitting model: XGBoost_r49_BAG_L2 ... Training model for up to 21287.12s of the 21286.96s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t34.09s\t = Training   runtime\n\t0.94s\t = Validation runtime\nFitting model: CatBoost_r5_BAG_L2 ... Training model for up to 21251.38s of the 21251.22s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t28.04s\t = Training   runtime\n\t0.36s\t = Validation runtime\nFitting model: CatBoost_r143_BAG_L2 ... Training model for up to 21222.29s of the 21222.12s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t28.18s\t = Training   runtime\n\t0.31s\t = Validation runtime\nFitting model: ExtraTrees_r178_BAG_L2 ... Training model for up to 21192.92s of the 21192.75s of remaining time.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t120.86s\t = Training   runtime\n\t7.18s\t = Validation runtime\nFitting model: XGBoost_r31_BAG_L2 ... Training model for up to 21063.72s of the 21063.55s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t48.91s\t = Training   runtime\n\t1.06s\t = Validation runtime\nFitting model: CatBoost_r60_BAG_L2 ... Training model for up to 21013.0s of the 21012.84s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t30.92s\t = Training   runtime\n\t0.55s\t = Validation runtime\nFitting model: LightGBM_r135_BAG_L2 ... Training model for up to 20980.83s of the 20980.66s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t119.12s\t = Training   runtime\n\t1.2s\t = Validation runtime\nFitting model: XGBoost_r22_BAG_L2 ... Training model for up to 20858.96s of the 20858.8s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t25.41s\t = Training   runtime\n\t0.84s\t = Validation runtime\nFitting model: CatBoost_r6_BAG_L2 ... Training model for up to 20832.03s of the 20831.86s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t27.54s\t = Training   runtime\n\t0.4s\t = Validation runtime\nFitting model: LightGBM_r121_BAG_L2 ... Training model for up to 20803.41s of the 20803.24s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's l2: 0.0216308\tvalid_set's mean_squared_log_error: -0.105996\n","output_type":"stream"},{"name":"stderr","text":"\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t289.53s\t = Training   runtime\n\t3.31s\t = Validation runtime\nFitting model: CatBoost_r180_BAG_L2 ... Training model for up to 20507.53s of the 20507.37s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t34.22s\t = Training   runtime\n\t0.51s\t = Validation runtime\nFitting model: ExtraTrees_r197_BAG_L2 ... Training model for up to 20472.08s of the 20471.91s of remaining time.\n\t-0.0446\t = Validation score   (-mean_squared_log_error)\n\t271.16s\t = Training   runtime\n\t10.02s\t = Validation runtime\nFitting model: CatBoost_r12_BAG_L2 ... Training model for up to 20188.15s of the 20187.98s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t36.24s\t = Training   runtime\n\t0.36s\t = Validation runtime\nFitting model: ExtraTrees_r126_BAG_L2 ... Training model for up to 20150.87s of the 20150.7s of remaining time.\n\t-0.0444\t = Validation score   (-mean_squared_log_error)\n\t26.62s\t = Training   runtime\n\t8.57s\t = Validation runtime\nFitting model: CatBoost_r163_BAG_L2 ... Training model for up to 20113.57s of the 20113.4s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t32.55s\t = Training   runtime\n\t0.33s\t = Validation runtime\nFitting model: CatBoost_r198_BAG_L2 ... Training model for up to 20079.99s of the 20079.83s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t31.39s\t = Training   runtime\n\t0.35s\t = Validation runtime\nFitting model: XGBoost_r95_BAG_L2 ... Training model for up to 20047.34s of the 20047.17s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0443\t = Validation score   (-mean_squared_log_error)\n\t25.5s\t = Training   runtime\n\t1.11s\t = Validation runtime\nFitting model: XGBoost_r34_BAG_L2 ... Training model for up to 20019.84s of the 20019.68s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t65.47s\t = Training   runtime\n\t1.03s\t = Validation runtime\nFitting model: LightGBM_r42_BAG_L2 ... Training model for up to 19952.5s of the 19952.33s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n\t-0.0445\t = Validation score   (-mean_squared_log_error)\n\t48.72s\t = Training   runtime\n\t0.51s\t = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 2401.58s of the 19901.81s of remaining time.\n\tEnsemble Weights: {'CatBoost_r50_BAG_L2': 0.167, 'LightGBM_r15_BAG_L2': 0.167, 'ExtraTrees_r126_BAG_L2': 0.167, 'LightGBM_r96_BAG_L2': 0.083, 'ExtraTrees_r172_BAG_L2': 0.083, 'ExtraTrees_r49_BAG_L2': 0.083, 'LightGBM_r94_BAG_L2': 0.083, 'ExtraTrees_r4_BAG_L2': 0.083, 'CatBoost_r6_BAG_L2': 0.083}\n\t-0.0442\t = Validation score   (-mean_squared_log_error)\n\t0.94s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 8899.22s ... Best model: \"WeightedEnsemble_L3\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240518_100340\")\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<autogluon.tabular.predictor.predictor.TabularPredictor at 0x7bbcbf7c2230>"},"metadata":{}}]},{"cell_type":"code","source":"automl.leaderboard()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:32:00.000874Z","iopub.execute_input":"2024-05-18T12:32:00.001122Z","iopub.status.idle":"2024-05-18T12:32:00.096735Z","shell.execute_reply.started":"2024-05-18T12:32:00.001100Z","shell.execute_reply":"2024-05-18T12:32:00.095880Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                      model  score_val             eval_metric  pred_time_val     fit_time  \\\n0       WeightedEnsemble_L3  -0.044226  mean_squared_log_error     123.081263  5132.156023   \n1       WeightedEnsemble_L2  -0.044238  mean_squared_log_error      22.869454  1392.536815   \n2       CatBoost_r50_BAG_L2  -0.044268  mean_squared_log_error      90.716573  4679.708031   \n3        CatBoost_r6_BAG_L2  -0.044272  mean_squared_log_error      90.741764  4679.054069   \n4       CatBoost_r70_BAG_L2  -0.044280  mean_squared_log_error      90.888059  4691.888458   \n5       LightGBM_r15_BAG_L2  -0.044280  mean_squared_log_error      90.917255  4690.889100   \n6      CatBoost_r180_BAG_L2  -0.044287  mean_squared_log_error      90.858963  4685.743432   \n7        XGBoost_r31_BAG_L2  -0.044290  mean_squared_log_error      91.402729  4700.428930   \n8       CatBoost_r13_BAG_L2  -0.044292  mean_squared_log_error      90.713612  4702.388487   \n9       LightGBM_r96_BAG_L2  -0.044292  mean_squared_log_error      91.314396  4694.927181   \n10      CatBoost_r86_BAG_L2  -0.044293  mean_squared_log_error      90.683980  4710.389431   \n11     ExtraTrees_r4_BAG_L2  -0.044294  mean_squared_log_error      96.509603  4755.906535   \n12       CatBoost_r9_BAG_L2  -0.044295  mean_squared_log_error      91.019863  4700.326642   \n13      CatBoost_r12_BAG_L2  -0.044296  mean_squared_log_error      90.702698  4687.761754   \n14        LightGBMXT_BAG_L2  -0.044300  mean_squared_log_error      90.717034  4677.522113   \n15      LightGBM_r94_BAG_L2  -0.044300  mean_squared_log_error      90.761193  4676.987271   \n16          CatBoost_BAG_L2  -0.044304  mean_squared_log_error      90.696883  4699.549669   \n17     CatBoost_r128_BAG_L2  -0.044305  mean_squared_log_error      90.978115  4696.994566   \n18       XGBoost_r89_BAG_L2  -0.044309  mean_squared_log_error      91.151876  4675.200037   \n19          LightGBM_BAG_L2  -0.044310  mean_squared_log_error      90.672427  4678.886708   \n20     CatBoost_r167_BAG_L2  -0.044312  mean_squared_log_error      90.648875  4689.351900   \n21     CatBoost_r143_BAG_L2  -0.044312  mean_squared_log_error      90.651619  4679.703651   \n22     CatBoost_r198_BAG_L2  -0.044312  mean_squared_log_error      90.699069  4682.910899   \n23      CatBoost_r69_BAG_L2  -0.044313  mean_squared_log_error      90.689915  4695.555302   \n24       XGBoost_r95_BAG_L2  -0.044314  mean_squared_log_error      91.455490  4677.022226   \n25      CatBoost_r60_BAG_L2  -0.044315  mean_squared_log_error      90.895709  4682.435813   \n26     LightGBM_r131_BAG_L2  -0.044316  mean_squared_log_error      91.998824  4750.865774   \n27     CatBoost_r177_BAG_L2  -0.044316  mean_squared_log_error      90.662963  4679.133668   \n28     LightGBM_r121_BAG_L1  -0.044319  mean_squared_log_error       3.229946   209.509833   \n29     LightGBM_r161_BAG_L1  -0.044324  mean_squared_log_error       3.994895   266.754714   \n30      CatBoost_r49_BAG_L2  -0.044328  mean_squared_log_error      90.647921  4679.594688   \n31     CatBoost_r163_BAG_L2  -0.044332  mean_squared_log_error      90.676819  4684.072997   \n32      LightGBM_r30_BAG_L2  -0.044332  mean_squared_log_error      93.173195  4800.477240   \n33       CatBoost_r5_BAG_L2  -0.044333  mean_squared_log_error      90.703212  4679.557630   \n34       XGBoost_r22_BAG_L2  -0.044333  mean_squared_log_error      91.184969  4676.930475   \n35     CatBoost_r137_BAG_L2  -0.044338  mean_squared_log_error      90.687813  4682.037182   \n36     LightGBM_r130_BAG_L1  -0.044352  mean_squared_log_error       0.616815    41.733607   \n37           XGBoost_BAG_L2  -0.044354  mean_squared_log_error      91.152267  4676.167595   \n38       XGBoost_r98_BAG_L2  -0.044359  mean_squared_log_error      91.050904  4797.617683   \n39     LightGBM_r121_BAG_L2  -0.044360  mean_squared_log_error      93.656554  4941.046542   \n40     LightGBM_r188_BAG_L2  -0.044361  mean_squared_log_error      91.026630  4712.120552   \n41       XGBoost_r49_BAG_L2  -0.044371  mean_squared_log_error      91.285565  4685.609639   \n42       CatBoost_r9_BAG_L1  -0.044373  mean_squared_log_error       1.174226    90.097077   \n43       XGBoost_r98_BAG_L1  -0.044377  mean_squared_log_error       0.584687   207.337070   \n44   ExtraTrees_r126_BAG_L2  -0.044384  mean_squared_log_error      98.918602  4678.142816   \n45     LightGBM_r161_BAG_L2  -0.044386  mean_squared_log_error      94.132323  4991.602967   \n46     LightGBM_r130_BAG_L2  -0.044390  mean_squared_log_error      90.786640  4692.065406   \n47     LightGBM_r135_BAG_L1  -0.044400  mean_squared_log_error       1.019711    84.222804   \n48     CatBoost_r128_BAG_L1  -0.044401  mean_squared_log_error       0.917148    72.074229   \n49     LightGBM_r143_BAG_L1  -0.044403  mean_squared_log_error       2.786677   195.572795   \n50      CatBoost_r70_BAG_L1  -0.044405  mean_squared_log_error       1.302974    98.071484   \n51      XGBoost_r194_BAG_L2  -0.044406  mean_squared_log_error      90.732036  4667.949614   \n52   ExtraTrees_r172_BAG_L2  -0.044409  mean_squared_log_error      97.398161  4807.729802   \n53     LightGBMLarge_BAG_L2  -0.044409  mean_squared_log_error      91.064112  4744.506512   \n54   ExtraTrees_r178_BAG_L2  -0.044411  mean_squared_log_error      97.519903  4772.374575   \n55     LightGBM_r143_BAG_L2  -0.044416  mean_squared_log_error      92.563573  4854.591729   \n56       XGBoost_r49_BAG_L1  -0.044422  mean_squared_log_error       0.507759    35.122969   \n57     CatBoost_r180_BAG_L1  -0.044425  mean_squared_log_error       0.867985    58.749537   \n58      CatBoost_r13_BAG_L1  -0.044432  mean_squared_log_error       0.484684   150.035619   \n59       XGBoost_r33_BAG_L2  -0.044435  mean_squared_log_error      91.522418  4752.015929   \n60      CatBoost_r86_BAG_L1  -0.044444  mean_squared_log_error       0.391637   212.233780   \n61      CatBoost_r12_BAG_L1  -0.044448  mean_squared_log_error       0.353591   100.317435   \n62     LightGBMLarge_BAG_L1  -0.044450  mean_squared_log_error       0.715516    68.805076   \n63          CatBoost_BAG_L1  -0.044451  mean_squared_log_error       0.324120   179.376867   \n64       XGBoost_r34_BAG_L2  -0.044454  mean_squared_log_error      91.376269  4716.989908   \n65     LightGBM_r131_BAG_L1  -0.044456  mean_squared_log_error       1.376338    70.543333   \n66       XGBoost_r31_BAG_L1  -0.044462  mean_squared_log_error       1.075324   232.909784   \n67      LightGBM_r42_BAG_L2  -0.044467  mean_squared_log_error      90.856533  4700.236133   \n68     CatBoost_r198_BAG_L1  -0.044471  mean_squared_log_error       0.337661    80.995867   \n69     CatBoost_r143_BAG_L1  -0.044479  mean_squared_log_error       0.308028    61.052596   \n70      CatBoost_r60_BAG_L1  -0.044480  mean_squared_log_error       0.322509    82.401421   \n71     CatBoost_r177_BAG_L1  -0.044481  mean_squared_log_error       0.328797    63.700651   \n72       XGBoost_r95_BAG_L1  -0.044481  mean_squared_log_error       0.526726    28.331206   \n73     LightGBM_r135_BAG_L2  -0.044482  mean_squared_log_error      91.548989  4770.634869   \n74    ExtraTrees_r49_BAG_L2  -0.044484  mean_squared_log_error      98.554187  4680.023216   \n75       XGBoost_r22_BAG_L1  -0.044488  mean_squared_log_error       0.474222    21.734829   \n76     CatBoost_r167_BAG_L1  -0.044497  mean_squared_log_error       0.291116    99.542440   \n77      CatBoost_r69_BAG_L1  -0.044499  mean_squared_log_error       0.357597   170.165123   \n78      CatBoost_r50_BAG_L1  -0.044503  mean_squared_log_error       0.968180    78.884232   \n79     LightGBM_r188_BAG_L1  -0.044506  mean_squared_log_error       1.023504    65.864485   \n80      XGBoost_r194_BAG_L1  -0.044515  mean_squared_log_error       0.247974    14.873227   \n81       XGBoost_r89_BAG_L1  -0.044515  mean_squared_log_error       0.506024    25.930381   \n82           XGBoost_BAG_L1  -0.044518  mean_squared_log_error       0.464000    20.663250   \n83     CatBoost_r163_BAG_L1  -0.044522  mean_squared_log_error       0.271304    75.815803   \n84     CatBoost_r137_BAG_L1  -0.044524  mean_squared_log_error       0.391600    90.248886   \n85       CatBoost_r6_BAG_L1  -0.044526  mean_squared_log_error       0.856265    68.963606   \n86       XGBoost_r34_BAG_L1  -0.044544  mean_squared_log_error       0.530716    57.790061   \n87     ExtraTreesMSE_BAG_L2  -0.044547  mean_squared_log_error      97.870298  4861.361773   \n88       CatBoost_r5_BAG_L1  -0.044548  mean_squared_log_error       0.355151    78.961467   \n89       XGBoost_r33_BAG_L1  -0.044552  mean_squared_log_error       0.602118    97.074397   \n90    ExtraTrees_r42_BAG_L2  -0.044560  mean_squared_log_error      98.129743  4816.735838   \n91      LightGBM_r30_BAG_L1  -0.044561  mean_squared_log_error       3.012915   127.485955   \n92      CatBoost_r49_BAG_L1  -0.044566  mean_squared_log_error       0.336266    71.587264   \n93          LightGBM_BAG_L1  -0.044568  mean_squared_log_error       0.393867    21.146977   \n94   ExtraTrees_r197_BAG_L2  -0.044569  mean_squared_log_error     100.368102  4922.679328   \n95      LightGBM_r15_BAG_L1  -0.044621  mean_squared_log_error       0.848342    37.529264   \n96      LightGBM_r42_BAG_L1  -0.044624  mean_squared_log_error       0.839121    58.382992   \n97     ExtraTrees_r4_BAG_L1  -0.044646  mean_squared_log_error       4.290641    42.595846   \n98      LightGBM_r96_BAG_L1  -0.044650  mean_squared_log_error       5.652044   129.952940   \n99        LightGBMXT_BAG_L1  -0.044674  mean_squared_log_error       0.714516    34.439106   \n100  ExtraTrees_r178_BAG_L1  -0.044685  mean_squared_log_error       4.956555    44.079998   \n101  ExtraTrees_r172_BAG_L1  -0.044689  mean_squared_log_error       5.730132    64.885235   \n102     LightGBM_r94_BAG_L1  -0.044689  mean_squared_log_error       1.401058    40.483725   \n103    ExtraTreesMSE_BAG_L1  -0.044974  mean_squared_log_error       5.614438   100.521916   \n104   ExtraTrees_r42_BAG_L1  -0.045005  mean_squared_log_error       6.342125    73.884777   \n105  ExtraTrees_r126_BAG_L1  -0.045038  mean_squared_log_error       5.853443    14.897331   \n106  ExtraTrees_r197_BAG_L1  -0.045110  mean_squared_log_error       6.789758    99.803066   \n107   ExtraTrees_r49_BAG_L1  -0.045147  mean_squared_log_error       6.573746    19.768321   \n108    LightGBM_r196_BAG_L2  -0.086778  mean_squared_log_error      90.501552  4671.651289   \n109    LightGBM_r196_BAG_L1  -0.086803  mean_squared_log_error       0.107633    13.612411   \n\n     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order  \n0                  0.002524           0.940536            3       True        110  \n1                  0.002479           0.971074            2       True         55  \n2                  0.372448          28.188964            2       True         73  \n3                  0.397640          27.535002            2       True         99  \n4                  0.543935          40.369392            2       True         78  \n5                  0.573131          39.370033            2       True         82  \n6                  0.514838          34.224365            2       True        101  \n7                  1.058604          48.909863            2       True         95  \n8                  0.369488          50.869421            2       True         69  \n9                  0.970271          43.408114            2       True         65  \n10                 0.339856          58.870364            2       True         83  \n11                 6.165478         104.387469            2       True         89  \n12                 0.675739          48.807575            2       True         64  \n13                 0.358574          36.242688            2       True        103  \n14                 0.372909          26.003046            2       True         56  \n15                 0.417069          25.468204            2       True         87  \n16                 0.352759          48.030602            2       True         58  \n17                 0.633991          45.475499            2       True         88  \n18                 0.807752          23.680971            2       True         71  \n19                 0.328303          27.367641            2       True         57  \n20                 0.304750          37.832834            2       True         80  \n21                 0.307495          28.184584            2       True         93  \n22                 0.354944          31.391832            2       True        106  \n23                 0.345791          44.036235            2       True         76  \n24                 1.111366          25.503159            2       True        107  \n25                 0.551584          30.916747            2       True         96  \n26                 1.654700          99.346708            2       True         63  \n27                 0.318839          27.614601            2       True         62  \n28                 3.229946         209.509833            1       True         45  \n29                 3.994895         266.754714            1       True         22  \n30                 0.303797          28.075621            2       True         84  \n31                 0.332695          32.553930            2       True        105  \n32                 2.829071         148.958174            2       True         90  \n33                 0.359087          28.038563            2       True         92  \n34                 0.840845          25.411409            2       True         98  \n35                 0.343689          30.518116            2       True         68  \n36                 0.616815          41.733607            1       True         17  \n37                 0.808142          24.648528            2       True         60  \n38                 0.706780         146.098616            2       True         81  \n39                 3.312430         289.527475            2       True        100  \n40                 0.682505          60.601485            2       True         70  \n41                 0.941440          34.090572            2       True         91  \n42                 1.174226          90.097077            1       True          9  \n43                 0.584687         207.337070            1       True         26  \n44                 8.574478          26.623749            2       True        104  \n45                 3.788199         340.083901            2       True         77  \n46                 0.442515          40.546339            2       True         72  \n47                 1.019711          84.222804            1       True         42  \n48                 0.917148          72.074229            1       True         33  \n49                 2.786677         195.572795            1       True         31  \n50                 1.302974          98.071484            1       True         23  \n51                 0.387912          16.430547            2       True         74  \n52                 7.054037         156.210736            2       True         75  \n53                 0.719988          92.987446            2       True         61  \n54                 7.175779         120.855508            2       True         94  \n55                 2.219449         203.072662            2       True         86  \n56                 0.507759          35.122969            1       True         36  \n57                 0.867985          58.749537            1       True         46  \n58                 0.484684         150.035619            1       True         14  \n59                 1.178294         100.496862            2       True         66  \n60                 0.391637         212.233780            1       True         28  \n61                 0.353591         100.317435            1       True         48  \n62                 0.715516          68.805076            1       True          6  \n63                 0.324120         179.376867            1       True          3  \n64                 1.032145          65.470841            2       True        108  \n65                 1.376338          70.543333            1       True          8  \n66                 1.075324         232.909784            1       True         40  \n67                 0.512409          48.717067            2       True        109  \n68                 0.337661          80.995867            1       True         51  \n69                 0.308028          61.052596            1       True         38  \n70                 0.322509          82.401421            1       True         41  \n71                 0.328797          63.700651            1       True          7  \n72                 0.526726          28.331206            1       True         52  \n73                 1.204864         119.115802            2       True         97  \n74                 8.210063          28.504149            2       True         85  \n75                 0.474222          21.734829            1       True         43  \n76                 0.291116          99.542440            1       True         25  \n77                 0.357597         170.165123            1       True         21  \n78                 0.968180          78.884232            1       True         18  \n79                 1.023504          65.864485            1       True         15  \n80                 0.247974          14.873227            1       True         19  \n81                 0.506024          25.930381            1       True         16  \n82                 0.464000          20.663250            1       True          5  \n83                 0.271304          75.815803            1       True         50  \n84                 0.391600          90.248886            1       True         13  \n85                 0.856265          68.963606            1       True         44  \n86                 0.530716          57.790061            1       True         53  \n87                 7.526174         209.842707            2       True         59  \n88                 0.355151          78.961467            1       True         37  \n89                 0.602118          97.074397            1       True         11  \n90                 7.785619         165.216772            2       True         67  \n91                 3.012915         127.485955            1       True         35  \n92                 0.336266          71.587264            1       True         29  \n93                 0.393867          21.146977            1       True          2  \n94                10.023977         271.160262            2       True        102  \n95                 0.848342          37.529264            1       True         27  \n96                 0.839121          58.382992            1       True         54  \n97                 4.290641          42.595846            1       True         34  \n98                 5.652044         129.952940            1       True         10  \n99                 0.714516          34.439106            1       True          1  \n100                4.956555          44.079998            1       True         39  \n101                5.730132          64.885235            1       True         20  \n102                1.401058          40.483725            1       True         32  \n103                5.614438         100.521916            1       True          4  \n104                6.342125          73.884777            1       True         12  \n105                5.853443          14.897331            1       True         49  \n106                6.789758          99.803066            1       True         47  \n107                6.573746          19.768321            1       True         30  \n108                0.157428          20.132222            2       True         79  \n109                0.107633          13.612411            1       True         24  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WeightedEnsemble_L3</td>\n      <td>-0.044226</td>\n      <td>mean_squared_log_error</td>\n      <td>123.081263</td>\n      <td>5132.156023</td>\n      <td>0.002524</td>\n      <td>0.940536</td>\n      <td>3</td>\n      <td>True</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>-0.044238</td>\n      <td>mean_squared_log_error</td>\n      <td>22.869454</td>\n      <td>1392.536815</td>\n      <td>0.002479</td>\n      <td>0.971074</td>\n      <td>2</td>\n      <td>True</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CatBoost_r50_BAG_L2</td>\n      <td>-0.044268</td>\n      <td>mean_squared_log_error</td>\n      <td>90.716573</td>\n      <td>4679.708031</td>\n      <td>0.372448</td>\n      <td>28.188964</td>\n      <td>2</td>\n      <td>True</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CatBoost_r6_BAG_L2</td>\n      <td>-0.044272</td>\n      <td>mean_squared_log_error</td>\n      <td>90.741764</td>\n      <td>4679.054069</td>\n      <td>0.397640</td>\n      <td>27.535002</td>\n      <td>2</td>\n      <td>True</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CatBoost_r70_BAG_L2</td>\n      <td>-0.044280</td>\n      <td>mean_squared_log_error</td>\n      <td>90.888059</td>\n      <td>4691.888458</td>\n      <td>0.543935</td>\n      <td>40.369392</td>\n      <td>2</td>\n      <td>True</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>LightGBM_r15_BAG_L2</td>\n      <td>-0.044280</td>\n      <td>mean_squared_log_error</td>\n      <td>90.917255</td>\n      <td>4690.889100</td>\n      <td>0.573131</td>\n      <td>39.370033</td>\n      <td>2</td>\n      <td>True</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CatBoost_r180_BAG_L2</td>\n      <td>-0.044287</td>\n      <td>mean_squared_log_error</td>\n      <td>90.858963</td>\n      <td>4685.743432</td>\n      <td>0.514838</td>\n      <td>34.224365</td>\n      <td>2</td>\n      <td>True</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>XGBoost_r31_BAG_L2</td>\n      <td>-0.044290</td>\n      <td>mean_squared_log_error</td>\n      <td>91.402729</td>\n      <td>4700.428930</td>\n      <td>1.058604</td>\n      <td>48.909863</td>\n      <td>2</td>\n      <td>True</td>\n      <td>95</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>CatBoost_r13_BAG_L2</td>\n      <td>-0.044292</td>\n      <td>mean_squared_log_error</td>\n      <td>90.713612</td>\n      <td>4702.388487</td>\n      <td>0.369488</td>\n      <td>50.869421</td>\n      <td>2</td>\n      <td>True</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>LightGBM_r96_BAG_L2</td>\n      <td>-0.044292</td>\n      <td>mean_squared_log_error</td>\n      <td>91.314396</td>\n      <td>4694.927181</td>\n      <td>0.970271</td>\n      <td>43.408114</td>\n      <td>2</td>\n      <td>True</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CatBoost_r86_BAG_L2</td>\n      <td>-0.044293</td>\n      <td>mean_squared_log_error</td>\n      <td>90.683980</td>\n      <td>4710.389431</td>\n      <td>0.339856</td>\n      <td>58.870364</td>\n      <td>2</td>\n      <td>True</td>\n      <td>83</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ExtraTrees_r4_BAG_L2</td>\n      <td>-0.044294</td>\n      <td>mean_squared_log_error</td>\n      <td>96.509603</td>\n      <td>4755.906535</td>\n      <td>6.165478</td>\n      <td>104.387469</td>\n      <td>2</td>\n      <td>True</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>CatBoost_r9_BAG_L2</td>\n      <td>-0.044295</td>\n      <td>mean_squared_log_error</td>\n      <td>91.019863</td>\n      <td>4700.326642</td>\n      <td>0.675739</td>\n      <td>48.807575</td>\n      <td>2</td>\n      <td>True</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>CatBoost_r12_BAG_L2</td>\n      <td>-0.044296</td>\n      <td>mean_squared_log_error</td>\n      <td>90.702698</td>\n      <td>4687.761754</td>\n      <td>0.358574</td>\n      <td>36.242688</td>\n      <td>2</td>\n      <td>True</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>LightGBMXT_BAG_L2</td>\n      <td>-0.044300</td>\n      <td>mean_squared_log_error</td>\n      <td>90.717034</td>\n      <td>4677.522113</td>\n      <td>0.372909</td>\n      <td>26.003046</td>\n      <td>2</td>\n      <td>True</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>LightGBM_r94_BAG_L2</td>\n      <td>-0.044300</td>\n      <td>mean_squared_log_error</td>\n      <td>90.761193</td>\n      <td>4676.987271</td>\n      <td>0.417069</td>\n      <td>25.468204</td>\n      <td>2</td>\n      <td>True</td>\n      <td>87</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>CatBoost_BAG_L2</td>\n      <td>-0.044304</td>\n      <td>mean_squared_log_error</td>\n      <td>90.696883</td>\n      <td>4699.549669</td>\n      <td>0.352759</td>\n      <td>48.030602</td>\n      <td>2</td>\n      <td>True</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>CatBoost_r128_BAG_L2</td>\n      <td>-0.044305</td>\n      <td>mean_squared_log_error</td>\n      <td>90.978115</td>\n      <td>4696.994566</td>\n      <td>0.633991</td>\n      <td>45.475499</td>\n      <td>2</td>\n      <td>True</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>XGBoost_r89_BAG_L2</td>\n      <td>-0.044309</td>\n      <td>mean_squared_log_error</td>\n      <td>91.151876</td>\n      <td>4675.200037</td>\n      <td>0.807752</td>\n      <td>23.680971</td>\n      <td>2</td>\n      <td>True</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>LightGBM_BAG_L2</td>\n      <td>-0.044310</td>\n      <td>mean_squared_log_error</td>\n      <td>90.672427</td>\n      <td>4678.886708</td>\n      <td>0.328303</td>\n      <td>27.367641</td>\n      <td>2</td>\n      <td>True</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>CatBoost_r167_BAG_L2</td>\n      <td>-0.044312</td>\n      <td>mean_squared_log_error</td>\n      <td>90.648875</td>\n      <td>4689.351900</td>\n      <td>0.304750</td>\n      <td>37.832834</td>\n      <td>2</td>\n      <td>True</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>CatBoost_r143_BAG_L2</td>\n      <td>-0.044312</td>\n      <td>mean_squared_log_error</td>\n      <td>90.651619</td>\n      <td>4679.703651</td>\n      <td>0.307495</td>\n      <td>28.184584</td>\n      <td>2</td>\n      <td>True</td>\n      <td>93</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>CatBoost_r198_BAG_L2</td>\n      <td>-0.044312</td>\n      <td>mean_squared_log_error</td>\n      <td>90.699069</td>\n      <td>4682.910899</td>\n      <td>0.354944</td>\n      <td>31.391832</td>\n      <td>2</td>\n      <td>True</td>\n      <td>106</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>CatBoost_r69_BAG_L2</td>\n      <td>-0.044313</td>\n      <td>mean_squared_log_error</td>\n      <td>90.689915</td>\n      <td>4695.555302</td>\n      <td>0.345791</td>\n      <td>44.036235</td>\n      <td>2</td>\n      <td>True</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>XGBoost_r95_BAG_L2</td>\n      <td>-0.044314</td>\n      <td>mean_squared_log_error</td>\n      <td>91.455490</td>\n      <td>4677.022226</td>\n      <td>1.111366</td>\n      <td>25.503159</td>\n      <td>2</td>\n      <td>True</td>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>CatBoost_r60_BAG_L2</td>\n      <td>-0.044315</td>\n      <td>mean_squared_log_error</td>\n      <td>90.895709</td>\n      <td>4682.435813</td>\n      <td>0.551584</td>\n      <td>30.916747</td>\n      <td>2</td>\n      <td>True</td>\n      <td>96</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>LightGBM_r131_BAG_L2</td>\n      <td>-0.044316</td>\n      <td>mean_squared_log_error</td>\n      <td>91.998824</td>\n      <td>4750.865774</td>\n      <td>1.654700</td>\n      <td>99.346708</td>\n      <td>2</td>\n      <td>True</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>CatBoost_r177_BAG_L2</td>\n      <td>-0.044316</td>\n      <td>mean_squared_log_error</td>\n      <td>90.662963</td>\n      <td>4679.133668</td>\n      <td>0.318839</td>\n      <td>27.614601</td>\n      <td>2</td>\n      <td>True</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>LightGBM_r121_BAG_L1</td>\n      <td>-0.044319</td>\n      <td>mean_squared_log_error</td>\n      <td>3.229946</td>\n      <td>209.509833</td>\n      <td>3.229946</td>\n      <td>209.509833</td>\n      <td>1</td>\n      <td>True</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>LightGBM_r161_BAG_L1</td>\n      <td>-0.044324</td>\n      <td>mean_squared_log_error</td>\n      <td>3.994895</td>\n      <td>266.754714</td>\n      <td>3.994895</td>\n      <td>266.754714</td>\n      <td>1</td>\n      <td>True</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>CatBoost_r49_BAG_L2</td>\n      <td>-0.044328</td>\n      <td>mean_squared_log_error</td>\n      <td>90.647921</td>\n      <td>4679.594688</td>\n      <td>0.303797</td>\n      <td>28.075621</td>\n      <td>2</td>\n      <td>True</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>CatBoost_r163_BAG_L2</td>\n      <td>-0.044332</td>\n      <td>mean_squared_log_error</td>\n      <td>90.676819</td>\n      <td>4684.072997</td>\n      <td>0.332695</td>\n      <td>32.553930</td>\n      <td>2</td>\n      <td>True</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>LightGBM_r30_BAG_L2</td>\n      <td>-0.044332</td>\n      <td>mean_squared_log_error</td>\n      <td>93.173195</td>\n      <td>4800.477240</td>\n      <td>2.829071</td>\n      <td>148.958174</td>\n      <td>2</td>\n      <td>True</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>CatBoost_r5_BAG_L2</td>\n      <td>-0.044333</td>\n      <td>mean_squared_log_error</td>\n      <td>90.703212</td>\n      <td>4679.557630</td>\n      <td>0.359087</td>\n      <td>28.038563</td>\n      <td>2</td>\n      <td>True</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>XGBoost_r22_BAG_L2</td>\n      <td>-0.044333</td>\n      <td>mean_squared_log_error</td>\n      <td>91.184969</td>\n      <td>4676.930475</td>\n      <td>0.840845</td>\n      <td>25.411409</td>\n      <td>2</td>\n      <td>True</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>CatBoost_r137_BAG_L2</td>\n      <td>-0.044338</td>\n      <td>mean_squared_log_error</td>\n      <td>90.687813</td>\n      <td>4682.037182</td>\n      <td>0.343689</td>\n      <td>30.518116</td>\n      <td>2</td>\n      <td>True</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>LightGBM_r130_BAG_L1</td>\n      <td>-0.044352</td>\n      <td>mean_squared_log_error</td>\n      <td>0.616815</td>\n      <td>41.733607</td>\n      <td>0.616815</td>\n      <td>41.733607</td>\n      <td>1</td>\n      <td>True</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>XGBoost_BAG_L2</td>\n      <td>-0.044354</td>\n      <td>mean_squared_log_error</td>\n      <td>91.152267</td>\n      <td>4676.167595</td>\n      <td>0.808142</td>\n      <td>24.648528</td>\n      <td>2</td>\n      <td>True</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>XGBoost_r98_BAG_L2</td>\n      <td>-0.044359</td>\n      <td>mean_squared_log_error</td>\n      <td>91.050904</td>\n      <td>4797.617683</td>\n      <td>0.706780</td>\n      <td>146.098616</td>\n      <td>2</td>\n      <td>True</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>LightGBM_r121_BAG_L2</td>\n      <td>-0.044360</td>\n      <td>mean_squared_log_error</td>\n      <td>93.656554</td>\n      <td>4941.046542</td>\n      <td>3.312430</td>\n      <td>289.527475</td>\n      <td>2</td>\n      <td>True</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>LightGBM_r188_BAG_L2</td>\n      <td>-0.044361</td>\n      <td>mean_squared_log_error</td>\n      <td>91.026630</td>\n      <td>4712.120552</td>\n      <td>0.682505</td>\n      <td>60.601485</td>\n      <td>2</td>\n      <td>True</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>XGBoost_r49_BAG_L2</td>\n      <td>-0.044371</td>\n      <td>mean_squared_log_error</td>\n      <td>91.285565</td>\n      <td>4685.609639</td>\n      <td>0.941440</td>\n      <td>34.090572</td>\n      <td>2</td>\n      <td>True</td>\n      <td>91</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>CatBoost_r9_BAG_L1</td>\n      <td>-0.044373</td>\n      <td>mean_squared_log_error</td>\n      <td>1.174226</td>\n      <td>90.097077</td>\n      <td>1.174226</td>\n      <td>90.097077</td>\n      <td>1</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>XGBoost_r98_BAG_L1</td>\n      <td>-0.044377</td>\n      <td>mean_squared_log_error</td>\n      <td>0.584687</td>\n      <td>207.337070</td>\n      <td>0.584687</td>\n      <td>207.337070</td>\n      <td>1</td>\n      <td>True</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>ExtraTrees_r126_BAG_L2</td>\n      <td>-0.044384</td>\n      <td>mean_squared_log_error</td>\n      <td>98.918602</td>\n      <td>4678.142816</td>\n      <td>8.574478</td>\n      <td>26.623749</td>\n      <td>2</td>\n      <td>True</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>LightGBM_r161_BAG_L2</td>\n      <td>-0.044386</td>\n      <td>mean_squared_log_error</td>\n      <td>94.132323</td>\n      <td>4991.602967</td>\n      <td>3.788199</td>\n      <td>340.083901</td>\n      <td>2</td>\n      <td>True</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>LightGBM_r130_BAG_L2</td>\n      <td>-0.044390</td>\n      <td>mean_squared_log_error</td>\n      <td>90.786640</td>\n      <td>4692.065406</td>\n      <td>0.442515</td>\n      <td>40.546339</td>\n      <td>2</td>\n      <td>True</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>LightGBM_r135_BAG_L1</td>\n      <td>-0.044400</td>\n      <td>mean_squared_log_error</td>\n      <td>1.019711</td>\n      <td>84.222804</td>\n      <td>1.019711</td>\n      <td>84.222804</td>\n      <td>1</td>\n      <td>True</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>CatBoost_r128_BAG_L1</td>\n      <td>-0.044401</td>\n      <td>mean_squared_log_error</td>\n      <td>0.917148</td>\n      <td>72.074229</td>\n      <td>0.917148</td>\n      <td>72.074229</td>\n      <td>1</td>\n      <td>True</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>LightGBM_r143_BAG_L1</td>\n      <td>-0.044403</td>\n      <td>mean_squared_log_error</td>\n      <td>2.786677</td>\n      <td>195.572795</td>\n      <td>2.786677</td>\n      <td>195.572795</td>\n      <td>1</td>\n      <td>True</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>CatBoost_r70_BAG_L1</td>\n      <td>-0.044405</td>\n      <td>mean_squared_log_error</td>\n      <td>1.302974</td>\n      <td>98.071484</td>\n      <td>1.302974</td>\n      <td>98.071484</td>\n      <td>1</td>\n      <td>True</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>XGBoost_r194_BAG_L2</td>\n      <td>-0.044406</td>\n      <td>mean_squared_log_error</td>\n      <td>90.732036</td>\n      <td>4667.949614</td>\n      <td>0.387912</td>\n      <td>16.430547</td>\n      <td>2</td>\n      <td>True</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>ExtraTrees_r172_BAG_L2</td>\n      <td>-0.044409</td>\n      <td>mean_squared_log_error</td>\n      <td>97.398161</td>\n      <td>4807.729802</td>\n      <td>7.054037</td>\n      <td>156.210736</td>\n      <td>2</td>\n      <td>True</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>LightGBMLarge_BAG_L2</td>\n      <td>-0.044409</td>\n      <td>mean_squared_log_error</td>\n      <td>91.064112</td>\n      <td>4744.506512</td>\n      <td>0.719988</td>\n      <td>92.987446</td>\n      <td>2</td>\n      <td>True</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>ExtraTrees_r178_BAG_L2</td>\n      <td>-0.044411</td>\n      <td>mean_squared_log_error</td>\n      <td>97.519903</td>\n      <td>4772.374575</td>\n      <td>7.175779</td>\n      <td>120.855508</td>\n      <td>2</td>\n      <td>True</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>LightGBM_r143_BAG_L2</td>\n      <td>-0.044416</td>\n      <td>mean_squared_log_error</td>\n      <td>92.563573</td>\n      <td>4854.591729</td>\n      <td>2.219449</td>\n      <td>203.072662</td>\n      <td>2</td>\n      <td>True</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>XGBoost_r49_BAG_L1</td>\n      <td>-0.044422</td>\n      <td>mean_squared_log_error</td>\n      <td>0.507759</td>\n      <td>35.122969</td>\n      <td>0.507759</td>\n      <td>35.122969</td>\n      <td>1</td>\n      <td>True</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>CatBoost_r180_BAG_L1</td>\n      <td>-0.044425</td>\n      <td>mean_squared_log_error</td>\n      <td>0.867985</td>\n      <td>58.749537</td>\n      <td>0.867985</td>\n      <td>58.749537</td>\n      <td>1</td>\n      <td>True</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>CatBoost_r13_BAG_L1</td>\n      <td>-0.044432</td>\n      <td>mean_squared_log_error</td>\n      <td>0.484684</td>\n      <td>150.035619</td>\n      <td>0.484684</td>\n      <td>150.035619</td>\n      <td>1</td>\n      <td>True</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>XGBoost_r33_BAG_L2</td>\n      <td>-0.044435</td>\n      <td>mean_squared_log_error</td>\n      <td>91.522418</td>\n      <td>4752.015929</td>\n      <td>1.178294</td>\n      <td>100.496862</td>\n      <td>2</td>\n      <td>True</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>CatBoost_r86_BAG_L1</td>\n      <td>-0.044444</td>\n      <td>mean_squared_log_error</td>\n      <td>0.391637</td>\n      <td>212.233780</td>\n      <td>0.391637</td>\n      <td>212.233780</td>\n      <td>1</td>\n      <td>True</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>CatBoost_r12_BAG_L1</td>\n      <td>-0.044448</td>\n      <td>mean_squared_log_error</td>\n      <td>0.353591</td>\n      <td>100.317435</td>\n      <td>0.353591</td>\n      <td>100.317435</td>\n      <td>1</td>\n      <td>True</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>LightGBMLarge_BAG_L1</td>\n      <td>-0.044450</td>\n      <td>mean_squared_log_error</td>\n      <td>0.715516</td>\n      <td>68.805076</td>\n      <td>0.715516</td>\n      <td>68.805076</td>\n      <td>1</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>CatBoost_BAG_L1</td>\n      <td>-0.044451</td>\n      <td>mean_squared_log_error</td>\n      <td>0.324120</td>\n      <td>179.376867</td>\n      <td>0.324120</td>\n      <td>179.376867</td>\n      <td>1</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>XGBoost_r34_BAG_L2</td>\n      <td>-0.044454</td>\n      <td>mean_squared_log_error</td>\n      <td>91.376269</td>\n      <td>4716.989908</td>\n      <td>1.032145</td>\n      <td>65.470841</td>\n      <td>2</td>\n      <td>True</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>LightGBM_r131_BAG_L1</td>\n      <td>-0.044456</td>\n      <td>mean_squared_log_error</td>\n      <td>1.376338</td>\n      <td>70.543333</td>\n      <td>1.376338</td>\n      <td>70.543333</td>\n      <td>1</td>\n      <td>True</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>XGBoost_r31_BAG_L1</td>\n      <td>-0.044462</td>\n      <td>mean_squared_log_error</td>\n      <td>1.075324</td>\n      <td>232.909784</td>\n      <td>1.075324</td>\n      <td>232.909784</td>\n      <td>1</td>\n      <td>True</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>LightGBM_r42_BAG_L2</td>\n      <td>-0.044467</td>\n      <td>mean_squared_log_error</td>\n      <td>90.856533</td>\n      <td>4700.236133</td>\n      <td>0.512409</td>\n      <td>48.717067</td>\n      <td>2</td>\n      <td>True</td>\n      <td>109</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>CatBoost_r198_BAG_L1</td>\n      <td>-0.044471</td>\n      <td>mean_squared_log_error</td>\n      <td>0.337661</td>\n      <td>80.995867</td>\n      <td>0.337661</td>\n      <td>80.995867</td>\n      <td>1</td>\n      <td>True</td>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>CatBoost_r143_BAG_L1</td>\n      <td>-0.044479</td>\n      <td>mean_squared_log_error</td>\n      <td>0.308028</td>\n      <td>61.052596</td>\n      <td>0.308028</td>\n      <td>61.052596</td>\n      <td>1</td>\n      <td>True</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>CatBoost_r60_BAG_L1</td>\n      <td>-0.044480</td>\n      <td>mean_squared_log_error</td>\n      <td>0.322509</td>\n      <td>82.401421</td>\n      <td>0.322509</td>\n      <td>82.401421</td>\n      <td>1</td>\n      <td>True</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>CatBoost_r177_BAG_L1</td>\n      <td>-0.044481</td>\n      <td>mean_squared_log_error</td>\n      <td>0.328797</td>\n      <td>63.700651</td>\n      <td>0.328797</td>\n      <td>63.700651</td>\n      <td>1</td>\n      <td>True</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>XGBoost_r95_BAG_L1</td>\n      <td>-0.044481</td>\n      <td>mean_squared_log_error</td>\n      <td>0.526726</td>\n      <td>28.331206</td>\n      <td>0.526726</td>\n      <td>28.331206</td>\n      <td>1</td>\n      <td>True</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>LightGBM_r135_BAG_L2</td>\n      <td>-0.044482</td>\n      <td>mean_squared_log_error</td>\n      <td>91.548989</td>\n      <td>4770.634869</td>\n      <td>1.204864</td>\n      <td>119.115802</td>\n      <td>2</td>\n      <td>True</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>ExtraTrees_r49_BAG_L2</td>\n      <td>-0.044484</td>\n      <td>mean_squared_log_error</td>\n      <td>98.554187</td>\n      <td>4680.023216</td>\n      <td>8.210063</td>\n      <td>28.504149</td>\n      <td>2</td>\n      <td>True</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>XGBoost_r22_BAG_L1</td>\n      <td>-0.044488</td>\n      <td>mean_squared_log_error</td>\n      <td>0.474222</td>\n      <td>21.734829</td>\n      <td>0.474222</td>\n      <td>21.734829</td>\n      <td>1</td>\n      <td>True</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>CatBoost_r167_BAG_L1</td>\n      <td>-0.044497</td>\n      <td>mean_squared_log_error</td>\n      <td>0.291116</td>\n      <td>99.542440</td>\n      <td>0.291116</td>\n      <td>99.542440</td>\n      <td>1</td>\n      <td>True</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>CatBoost_r69_BAG_L1</td>\n      <td>-0.044499</td>\n      <td>mean_squared_log_error</td>\n      <td>0.357597</td>\n      <td>170.165123</td>\n      <td>0.357597</td>\n      <td>170.165123</td>\n      <td>1</td>\n      <td>True</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>CatBoost_r50_BAG_L1</td>\n      <td>-0.044503</td>\n      <td>mean_squared_log_error</td>\n      <td>0.968180</td>\n      <td>78.884232</td>\n      <td>0.968180</td>\n      <td>78.884232</td>\n      <td>1</td>\n      <td>True</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>LightGBM_r188_BAG_L1</td>\n      <td>-0.044506</td>\n      <td>mean_squared_log_error</td>\n      <td>1.023504</td>\n      <td>65.864485</td>\n      <td>1.023504</td>\n      <td>65.864485</td>\n      <td>1</td>\n      <td>True</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>XGBoost_r194_BAG_L1</td>\n      <td>-0.044515</td>\n      <td>mean_squared_log_error</td>\n      <td>0.247974</td>\n      <td>14.873227</td>\n      <td>0.247974</td>\n      <td>14.873227</td>\n      <td>1</td>\n      <td>True</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>XGBoost_r89_BAG_L1</td>\n      <td>-0.044515</td>\n      <td>mean_squared_log_error</td>\n      <td>0.506024</td>\n      <td>25.930381</td>\n      <td>0.506024</td>\n      <td>25.930381</td>\n      <td>1</td>\n      <td>True</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>XGBoost_BAG_L1</td>\n      <td>-0.044518</td>\n      <td>mean_squared_log_error</td>\n      <td>0.464000</td>\n      <td>20.663250</td>\n      <td>0.464000</td>\n      <td>20.663250</td>\n      <td>1</td>\n      <td>True</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>CatBoost_r163_BAG_L1</td>\n      <td>-0.044522</td>\n      <td>mean_squared_log_error</td>\n      <td>0.271304</td>\n      <td>75.815803</td>\n      <td>0.271304</td>\n      <td>75.815803</td>\n      <td>1</td>\n      <td>True</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>CatBoost_r137_BAG_L1</td>\n      <td>-0.044524</td>\n      <td>mean_squared_log_error</td>\n      <td>0.391600</td>\n      <td>90.248886</td>\n      <td>0.391600</td>\n      <td>90.248886</td>\n      <td>1</td>\n      <td>True</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>CatBoost_r6_BAG_L1</td>\n      <td>-0.044526</td>\n      <td>mean_squared_log_error</td>\n      <td>0.856265</td>\n      <td>68.963606</td>\n      <td>0.856265</td>\n      <td>68.963606</td>\n      <td>1</td>\n      <td>True</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>XGBoost_r34_BAG_L1</td>\n      <td>-0.044544</td>\n      <td>mean_squared_log_error</td>\n      <td>0.530716</td>\n      <td>57.790061</td>\n      <td>0.530716</td>\n      <td>57.790061</td>\n      <td>1</td>\n      <td>True</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>ExtraTreesMSE_BAG_L2</td>\n      <td>-0.044547</td>\n      <td>mean_squared_log_error</td>\n      <td>97.870298</td>\n      <td>4861.361773</td>\n      <td>7.526174</td>\n      <td>209.842707</td>\n      <td>2</td>\n      <td>True</td>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>CatBoost_r5_BAG_L1</td>\n      <td>-0.044548</td>\n      <td>mean_squared_log_error</td>\n      <td>0.355151</td>\n      <td>78.961467</td>\n      <td>0.355151</td>\n      <td>78.961467</td>\n      <td>1</td>\n      <td>True</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>XGBoost_r33_BAG_L1</td>\n      <td>-0.044552</td>\n      <td>mean_squared_log_error</td>\n      <td>0.602118</td>\n      <td>97.074397</td>\n      <td>0.602118</td>\n      <td>97.074397</td>\n      <td>1</td>\n      <td>True</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>ExtraTrees_r42_BAG_L2</td>\n      <td>-0.044560</td>\n      <td>mean_squared_log_error</td>\n      <td>98.129743</td>\n      <td>4816.735838</td>\n      <td>7.785619</td>\n      <td>165.216772</td>\n      <td>2</td>\n      <td>True</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>LightGBM_r30_BAG_L1</td>\n      <td>-0.044561</td>\n      <td>mean_squared_log_error</td>\n      <td>3.012915</td>\n      <td>127.485955</td>\n      <td>3.012915</td>\n      <td>127.485955</td>\n      <td>1</td>\n      <td>True</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>CatBoost_r49_BAG_L1</td>\n      <td>-0.044566</td>\n      <td>mean_squared_log_error</td>\n      <td>0.336266</td>\n      <td>71.587264</td>\n      <td>0.336266</td>\n      <td>71.587264</td>\n      <td>1</td>\n      <td>True</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>LightGBM_BAG_L1</td>\n      <td>-0.044568</td>\n      <td>mean_squared_log_error</td>\n      <td>0.393867</td>\n      <td>21.146977</td>\n      <td>0.393867</td>\n      <td>21.146977</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>ExtraTrees_r197_BAG_L2</td>\n      <td>-0.044569</td>\n      <td>mean_squared_log_error</td>\n      <td>100.368102</td>\n      <td>4922.679328</td>\n      <td>10.023977</td>\n      <td>271.160262</td>\n      <td>2</td>\n      <td>True</td>\n      <td>102</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>LightGBM_r15_BAG_L1</td>\n      <td>-0.044621</td>\n      <td>mean_squared_log_error</td>\n      <td>0.848342</td>\n      <td>37.529264</td>\n      <td>0.848342</td>\n      <td>37.529264</td>\n      <td>1</td>\n      <td>True</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>LightGBM_r42_BAG_L1</td>\n      <td>-0.044624</td>\n      <td>mean_squared_log_error</td>\n      <td>0.839121</td>\n      <td>58.382992</td>\n      <td>0.839121</td>\n      <td>58.382992</td>\n      <td>1</td>\n      <td>True</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>ExtraTrees_r4_BAG_L1</td>\n      <td>-0.044646</td>\n      <td>mean_squared_log_error</td>\n      <td>4.290641</td>\n      <td>42.595846</td>\n      <td>4.290641</td>\n      <td>42.595846</td>\n      <td>1</td>\n      <td>True</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>LightGBM_r96_BAG_L1</td>\n      <td>-0.044650</td>\n      <td>mean_squared_log_error</td>\n      <td>5.652044</td>\n      <td>129.952940</td>\n      <td>5.652044</td>\n      <td>129.952940</td>\n      <td>1</td>\n      <td>True</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>LightGBMXT_BAG_L1</td>\n      <td>-0.044674</td>\n      <td>mean_squared_log_error</td>\n      <td>0.714516</td>\n      <td>34.439106</td>\n      <td>0.714516</td>\n      <td>34.439106</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>ExtraTrees_r178_BAG_L1</td>\n      <td>-0.044685</td>\n      <td>mean_squared_log_error</td>\n      <td>4.956555</td>\n      <td>44.079998</td>\n      <td>4.956555</td>\n      <td>44.079998</td>\n      <td>1</td>\n      <td>True</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>ExtraTrees_r172_BAG_L1</td>\n      <td>-0.044689</td>\n      <td>mean_squared_log_error</td>\n      <td>5.730132</td>\n      <td>64.885235</td>\n      <td>5.730132</td>\n      <td>64.885235</td>\n      <td>1</td>\n      <td>True</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>LightGBM_r94_BAG_L1</td>\n      <td>-0.044689</td>\n      <td>mean_squared_log_error</td>\n      <td>1.401058</td>\n      <td>40.483725</td>\n      <td>1.401058</td>\n      <td>40.483725</td>\n      <td>1</td>\n      <td>True</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>ExtraTreesMSE_BAG_L1</td>\n      <td>-0.044974</td>\n      <td>mean_squared_log_error</td>\n      <td>5.614438</td>\n      <td>100.521916</td>\n      <td>5.614438</td>\n      <td>100.521916</td>\n      <td>1</td>\n      <td>True</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>ExtraTrees_r42_BAG_L1</td>\n      <td>-0.045005</td>\n      <td>mean_squared_log_error</td>\n      <td>6.342125</td>\n      <td>73.884777</td>\n      <td>6.342125</td>\n      <td>73.884777</td>\n      <td>1</td>\n      <td>True</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>ExtraTrees_r126_BAG_L1</td>\n      <td>-0.045038</td>\n      <td>mean_squared_log_error</td>\n      <td>5.853443</td>\n      <td>14.897331</td>\n      <td>5.853443</td>\n      <td>14.897331</td>\n      <td>1</td>\n      <td>True</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>ExtraTrees_r197_BAG_L1</td>\n      <td>-0.045110</td>\n      <td>mean_squared_log_error</td>\n      <td>6.789758</td>\n      <td>99.803066</td>\n      <td>6.789758</td>\n      <td>99.803066</td>\n      <td>1</td>\n      <td>True</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>ExtraTrees_r49_BAG_L1</td>\n      <td>-0.045147</td>\n      <td>mean_squared_log_error</td>\n      <td>6.573746</td>\n      <td>19.768321</td>\n      <td>6.573746</td>\n      <td>19.768321</td>\n      <td>1</td>\n      <td>True</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>LightGBM_r196_BAG_L2</td>\n      <td>-0.086778</td>\n      <td>mean_squared_log_error</td>\n      <td>90.501552</td>\n      <td>4671.651289</td>\n      <td>0.157428</td>\n      <td>20.132222</td>\n      <td>2</td>\n      <td>True</td>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>LightGBM_r196_BAG_L1</td>\n      <td>-0.086803</td>\n      <td>mean_squared_log_error</td>\n      <td>0.107633</td>\n      <td>13.612411</td>\n      <td>0.107633</td>\n      <td>13.612411</td>\n      <td>1</td>\n      <td>True</td>\n      <td>24</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"predictions = automl.predict(test_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:32:00.097696Z","iopub.execute_input":"2024-05-18T12:32:00.097977Z","iopub.status.idle":"2024-05-18T12:38:02.166453Z","shell.execute_reply.started":"2024-05-18T12:32:00.097953Z","shell.execute_reply":"2024-05-18T12:38:02.165516Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"submission = predictions.reset_index().rename(columns={0: 'Rings'})","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:38:02.169133Z","iopub.execute_input":"2024-05-18T12:38:02.169407Z","iopub.status.idle":"2024-05-18T12:38:02.176319Z","shell.execute_reply.started":"2024-05-18T12:38:02.169375Z","shell.execute_reply":"2024-05-18T12:38:02.175300Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"submission.Rings = np.expm1(submission.Rings)\nsubmission.Rings = np.clip(submission.Rings,1,29)\nsubmission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:38:02.177416Z","iopub.execute_input":"2024-05-18T12:38:02.177730Z","iopub.status.idle":"2024-05-18T12:38:02.318337Z","shell.execute_reply.started":"2024-05-18T12:38:02.177703Z","shell.execute_reply":"2024-05-18T12:38:02.317322Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}